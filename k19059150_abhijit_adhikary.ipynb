{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65cb790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from imp import reload\n",
    "import json\n",
    "import logging\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e009bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducability\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6529904",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc7e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTorchTensor:\n",
    "    '''\n",
    "        Transforms a numpy ndarray to a torch tensor of the supplied datatype\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return torch.tensor(input, dtype=self.dtype)\n",
    "\n",
    "class DatasetHepatic(Dataset):\n",
    "    '''\n",
    "        min = 24\n",
    "        max = 181\n",
    "        median = 49\n",
    "        mean = 69\n",
    "    '''\n",
    "\n",
    "    def __init__(self, run_mode='train',\n",
    "                 transform_image=None,\n",
    "                 transform_label=None,\n",
    "                 patch_size_normal=25,\n",
    "                 patch_size_low=19,\n",
    "                 patch_size_out=9,\n",
    "                 patch_low_factor=3,\n",
    "                 label_percentage=0.1,\n",
    "                 batch_size_inner=100,\n",
    "                 use_probabilistic=False,\n",
    "                 create_numpy_dataset=False,\n",
    "                 dataset_variant='nib',\n",
    "                 train_percentage=0.8,\n",
    "                 use_elastic_deformation=False,\n",
    "                 user_affine_transformation=False,\n",
    "                 num_controlpoints=20, sigma=5, rotation=10, scale=(0.90, 1.10), shear=(0.01, 0.02)\n",
    "                 ):\n",
    "\n",
    "        self.run_mode = run_mode\n",
    "        self.create_numpy_dataset_cond = create_numpy_dataset\n",
    "        self.dataset_variant = dataset_variant\n",
    "        self.patch_size_normal = patch_size_normal\n",
    "        self.patch_size_low = patch_size_low\n",
    "        self.patch_size_out = patch_size_out\n",
    "        self.patch_low_factor = patch_low_factor\n",
    "        self.batch_size_inner = batch_size_inner\n",
    "        self.train_percentage = train_percentage\n",
    "        self.patch_size_low_up = self.patch_size_low * self.patch_low_factor\n",
    "\n",
    "        self.label_percentage = label_percentage\n",
    "        self.use_probabilistic = use_probabilistic\n",
    "        self.fetch_filenames()\n",
    "        self.create_numpy_dataset()\n",
    "        self.use_elastic_deformation = use_elastic_deformation\n",
    "        self.use_affine_transformation = user_affine_transformation\n",
    "        self.elastic_deformation = ElasticDeformation(num_controlpoints=num_controlpoints, sigma=sigma)\n",
    "        self.affine_transformation = AffineTransformation(rotation=rotation, scale=scale, shear=shear)\n",
    "\n",
    "        if transform_image is None:\n",
    "            self.transform_image = transforms.Compose([\n",
    "                ToTorchTensor(dtype=torch.float32),\n",
    "                transforms.Normalize(mean=0.5, std=0.5)\n",
    "            ])\n",
    "        else:\n",
    "            self.transform_image = transform_image\n",
    "\n",
    "        if transform_label is None:\n",
    "            self.transform_label = transforms.Compose([\n",
    "                ToTorchTensor(torch.int64)\n",
    "            ])\n",
    "        else:\n",
    "            self.transform_label = transform_label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataset_variant == 'nib':\n",
    "            image = self.read_file_nib(self.filenames_image_nib[index])\n",
    "            label = self.read_file_nib(self.filenames_label_nib[index])\n",
    "        elif self.dataset_variant == 'npy':\n",
    "            image = self.read_file_npy(self.filenames_image_npy[index])\n",
    "            label = self.read_file_npy(self.filenames_label_npy[index])\n",
    "\n",
    "        image = self.transform_image(image)\n",
    "        label = self.transform_label(label)\n",
    "        if self.use_elastic_deformation:\n",
    "            image, label = self.elastic_deformation(image, label)\n",
    "        if self.use_affine_transformation:\n",
    "            image, label, _, _ = self.affine_transformation(image, label)\n",
    "\n",
    "        image = image.detach().numpy()\n",
    "        label = label.detach().numpy()\n",
    "\n",
    "        # index of the original filenames as in the dataset folders\n",
    "        index_filename = self.filenames_image_npy[index][25:28]\n",
    "\n",
    "        if self.run_mode in ['train', 'val']:\n",
    "            if self.batch_size_inner > 1:\n",
    "                image_patch_normal_stack = torch.zeros(\n",
    "                    (self.batch_size_inner, self.patch_size_normal, self.patch_size_normal, self.patch_size_normal),\n",
    "                    dtype=torch.float32)\n",
    "                image_patch_low_up_stack = torch.zeros(\n",
    "                    (self.batch_size_inner, self.patch_size_low_up, self.patch_size_low_up, self.patch_size_low_up),\n",
    "                    dtype=torch.float32)\n",
    "                label_patch_out_stack = torch.zeros(\n",
    "                    (self.batch_size_inner, self.patch_size_out, self.patch_size_out, self.patch_size_out),\n",
    "                    dtype=torch.int64)\n",
    "\n",
    "                for index_inner in range(self.batch_size_inner):\n",
    "                    # extract the three different patches of labels\n",
    "                    label_patch_normal, label_patch_low_up, label_patch_out = self.get_random_patch(label)\n",
    "\n",
    "                    # extract the three different patches of images\n",
    "                    image_patch_normal = self.get_3D_crop(image, self.coordinate_center, self.patch_size_normal)\n",
    "                    image_patch_low_up = self.get_3D_crop(image, self.coordinate_center, self.patch_size_low_up)\n",
    "                    image_patch_out = self.get_3D_crop(image, self.coordinate_center, self.patch_size_out)\n",
    "\n",
    "                    image_patch_normal_stack[index_inner] = torch.tensor(image_patch_normal, dtype=torch.float32).unsqueeze(0)\n",
    "                    image_patch_low_up_stack[index_inner] = torch.tensor(image_patch_low_up, dtype=torch.float32).unsqueeze(0)\n",
    "                    label_patch_out_stack[index_inner] = torch.tensor(label_patch_out, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "                return image_patch_normal_stack.unsqueeze(1), image_patch_low_up_stack.unsqueeze(1), label_patch_out_stack.unsqueeze(1)\n",
    "            else:\n",
    "                # extract the three different patches of labels\n",
    "                label_patch_normal, label_patch_low_up, label_patch_out = self.get_random_patch(label)\n",
    "\n",
    "                # extract the three different patches of images\n",
    "                image_patch_normal = self.get_3D_crop(image, self.coordinate_center, self.patch_size_normal)\n",
    "                image_patch_low_up = self.get_3D_crop(image, self.coordinate_center, self.patch_size_low_up)\n",
    "                image_patch_out = self.get_3D_crop(image, self.coordinate_center, self.patch_size_out)\n",
    "\n",
    "                return torch.tensor(image_patch_normal, dtype=torch.float32).unsqueeze(0), \\\n",
    "                       torch.tensor(image_patch_low_up, dtype=torch.float32).unsqueeze(0), \\\n",
    "                       torch.tensor(label_patch_out, dtype=torch.int64).unsqueeze(0)\n",
    "\n",
    "        elif self.run_mode == 'inference':\n",
    "            # TODO fix uneven dimensions, otherwise run with batch size = 1\n",
    "            image = self.transform_image(image)\n",
    "            return image, label, index_filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def create_numpy_dataset(self):\n",
    "\n",
    "        def convert_to_numpy_from_nib(target_dir, filenames):\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "            for filename in tqdm(filenames, leave=False):\n",
    "                data_np = nib.load(filename).get_fdata()\n",
    "\n",
    "                filename_new = f'{filename[11:-7]}.npy'\n",
    "                save_path = os.path.join(target_dir, filename_new)\n",
    "                np.save(save_path, data_np)\n",
    "\n",
    "        save_dir_train_im = 'imagesTrNP'\n",
    "        train_filenames_im = self.filenames_image_nib\n",
    "\n",
    "        save_dir_train_labels = 'labelsTrNP'\n",
    "        train_filenames_labels = self.filenames_label_nib\n",
    "\n",
    "        if self.create_numpy_dataset_cond:\n",
    "            convert_to_numpy_from_nib(target_dir=save_dir_train_im, filenames=train_filenames_im)\n",
    "            convert_to_numpy_from_nib(target_dir=save_dir_train_labels, filenames=train_filenames_labels)\n",
    "\n",
    "    def get_label_percentage(self, input, label):\n",
    "        '''\n",
    "            Returns the percentage of supplied label in the voxel\n",
    "        '''\n",
    "        eps = 1e-9\n",
    "        denominator = input.shape[0] * input.shape[1] * input.shape[2]\n",
    "        numerator = np.sum(np.where(input == label, 1, 0))\n",
    "\n",
    "        return numerator / (denominator + eps)\n",
    "\n",
    "    def get_rand_index_3D(self, input, height=512, width=512, depth=20, patch_size=57):\n",
    "        '''\n",
    "            Returns a random starting index (top-left) of a valid 3D volume\n",
    "        '''\n",
    "        patch_size_half = patch_size // 2\n",
    "\n",
    "        if self.use_probabilistic:\n",
    "            # nearby currently selected label\n",
    "            loop_condition = True\n",
    "            background_count = 0\n",
    "            while loop_condition:\n",
    "                # crop the image so that the patch does not go outside the area of the image\n",
    "                input_cropped = input[patch_size_half:height - patch_size_half, patch_size_half:width - patch_size_half,\n",
    "                                patch_size_half:depth - patch_size_half]\n",
    "                # all indices of the cropped image equal to the current selected label category\n",
    "                indices_all = np.array(np.where(input_cropped == self.current_selected_label))\n",
    "                # print(indices_all.shape[1])\n",
    "                if indices_all.shape[1] >= 1:\n",
    "                    selected_index_w = np.random.randint(indices_all.shape[1])\n",
    "                    selected_index = indices_all[:, selected_index_w]\n",
    "\n",
    "                    index_h, index_w, index_d = (\n",
    "                        selected_index[0] + patch_size_half, selected_index[1] + patch_size_half,\n",
    "                        selected_index[2] + patch_size_half)\n",
    "                    # index_h, index_w, index_d = selected_index\n",
    "                    loop_condition = False\n",
    "                else:\n",
    "                    # print('here')\n",
    "                    if background_count > 0:\n",
    "                        # if none of the other two labels are present in the image, randomly pick a coordinate\n",
    "                        index_h = np.random.randint(patch_size_half, height - patch_size_half)\n",
    "                        index_w = np.random.randint(patch_size_half, width - patch_size_half)\n",
    "                        index_d = np.random.randint(patch_size_half, depth - patch_size_half)\n",
    "                        loop_condition = False\n",
    "\n",
    "                    else:\n",
    "                        if self.current_selected_label == 1:\n",
    "                            self.current_selected_label = 2\n",
    "                            background_count += 1\n",
    "                        elif self.current_selected_label == 2:\n",
    "                            self.current_selected_label = 1\n",
    "                            background_count += 1\n",
    "                        loop_condition = True\n",
    "        else:\n",
    "            #  complete random\n",
    "            index_h = np.random.randint(patch_size_half, height - patch_size_half)\n",
    "            index_w = np.random.randint(patch_size_half, width - patch_size_half)\n",
    "            index_d = np.random.randint(patch_size_half, depth - patch_size_half)\n",
    "\n",
    "        return (index_h, index_w, index_d)\n",
    "\n",
    "    def get_3D_crop(self, input, coordinate, patch_size):\n",
    "        '''\n",
    "            Returns a 3D patch of an input 3D image given a valid top-left coordinate\n",
    "        '''\n",
    "        assert patch_size % 2 == 1, 'Patch size should be an odd number'\n",
    "        patch_size_half = patch_size // 2\n",
    "\n",
    "        if len(input.shape) == 3:\n",
    "            height, width, depth = input.shape\n",
    "\n",
    "        if depth <= self.patch_size_low * self.patch_low_factor:\n",
    "            temp_array = np.zeros((height, width, self.patch_size_low * self.patch_low_factor))\n",
    "            temp_array[:, :, :depth] = input\n",
    "            input = temp_array\n",
    "            depth = temp_array.shape[2]\n",
    "\n",
    "        return input[\n",
    "               coordinate[0] - patch_size_half: coordinate[0] + patch_size_half + 1,\n",
    "               coordinate[1] - patch_size_half: coordinate[1] + patch_size_half + 1,\n",
    "               coordinate[2] - patch_size_half: coordinate[2] + patch_size_half + 1,\n",
    "               ]\n",
    "\n",
    "    def set_probabilistic_label(self):\n",
    "        '''\n",
    "            Randomly with equal probability select one of the three labels to be the current label\n",
    "        '''\n",
    "        label_probability = np.random.rand()\n",
    "        mode = 'major'  # equal, biased\n",
    "        if mode == 'biased':\n",
    "            if label_probability > 0.5:\n",
    "                self.current_selected_label = 1\n",
    "            else:\n",
    "                self.current_selected_label = 2\n",
    "        elif mode == 'equal':\n",
    "            if label_probability > 0.66:\n",
    "                self.current_selected_label = 2\n",
    "            elif label_probability < 0.33:\n",
    "                self.current_selected_label = 1\n",
    "            else:\n",
    "                self.current_selected_label = 0\n",
    "        elif mode == 'major':\n",
    "            if label_probability > 0.45:\n",
    "                self.current_selected_label = 2\n",
    "            elif label_probability < 0.45:\n",
    "                self.current_selected_label = 1\n",
    "            else:\n",
    "                self.current_selected_label = 0\n",
    "\n",
    "    def get_random_patch(self, input):\n",
    "        '''\n",
    "            Returns a valid cubic sub-volume with edge lenth = patch_size from a supplied 3D input volume image_input\n",
    "        '''\n",
    "        # a = copy.deepcopy(input)\n",
    "        if len(input.shape) == 3:\n",
    "            height, width, depth = input.shape\n",
    "\n",
    "        if depth <= self.patch_size_low * self.patch_low_factor:\n",
    "            temp_array = np.zeros((height, width, self.patch_size_low * self.patch_low_factor))\n",
    "            temp_array[:, :, :depth] = input\n",
    "            input = temp_array\n",
    "            depth = temp_array.shape[2]\n",
    "\n",
    "        loop_condition = True\n",
    "        if self.use_probabilistic:\n",
    "            self.set_probabilistic_label()\n",
    "\n",
    "        # keep sampling a new patch until the current label meets the desired overall percentage\n",
    "        while loop_condition:\n",
    "            # get a valid coordinate and extract the patch\n",
    "            self.coordinate_center = self.get_rand_index_3D(input, height, width, depth, self.patch_size_low_up)\n",
    "\n",
    "            patch_normal = self.get_3D_crop(input, self.coordinate_center, self.patch_size_normal)\n",
    "            patch_low_up = self.get_3D_crop(input, self.coordinate_center, self.patch_size_low_up)\n",
    "            patch_out = self.get_3D_crop(input, self.coordinate_center, self.patch_size_out)\n",
    "\n",
    "            loop_condition = False\n",
    "\n",
    "        return patch_normal, patch_low_up, patch_out\n",
    "\n",
    "    def read_file_nib(self, filename):\n",
    "        '''\n",
    "            Reads a nibabel file and returns it in numpy ndarray format\n",
    "        '''\n",
    "        try:\n",
    "            data_nib = nib.load(filename).get_fdata()\n",
    "        except FileNotFoundError:\n",
    "            print(f'Error reading file: {filename}')\n",
    "\n",
    "        return data_nib\n",
    "\n",
    "    def read_file_npy(self, filename):\n",
    "        '''\n",
    "            Reads a npy file and returns it in numpy ndarray format\n",
    "        '''\n",
    "        try:\n",
    "            data_npy = np.load(filename)\n",
    "        except FileNotFoundError:\n",
    "            print(f'Error reading file: {filename}')\n",
    "\n",
    "        return data_npy\n",
    "\n",
    "    def fetch_filenames(self, path_meta='dataset.json'):\n",
    "        '''\n",
    "            Reads the dataset.json file and extracts the training and test image and/or labels\n",
    "        :return:\n",
    "        '''\n",
    "        try:\n",
    "            with open(path_meta) as file_meta:\n",
    "                data_meta = json.loads(file_meta.read())\n",
    "        except FileNotFoundError:\n",
    "            print(f'Meta file: {self.path_meta} not found')\n",
    "\n",
    "        num_samples = len(data_meta['training'])\n",
    "\n",
    "        if self.run_mode == 'train':\n",
    "            num_samples = int(np.floor(self.train_percentage * num_samples))\n",
    "\n",
    "            self.filenames_image_nib = [current_sample['image'] for current_sample in data_meta['training']][\n",
    "                                       :num_samples]\n",
    "            self.filenames_label_nib = [current_sample['label'] for current_sample in data_meta['training']][\n",
    "                                       :num_samples]\n",
    "\n",
    "            self.filenames_image_npy = [os.path.join('.', 'imagesTrNP', f'{filename[11:-7]}.npy') for filename in\n",
    "                                        self.filenames_image_nib]\n",
    "            self.filenames_label_npy = [os.path.join('.', 'labelsTrNP', f'{filename[11:-7]}.npy') for filename in\n",
    "                                        self.filenames_label_nib]\n",
    "            self.num_samples = num_samples\n",
    "        else:\n",
    "            num_train = int(np.floor(self.train_percentage * num_samples))\n",
    "\n",
    "            self.filenames_image_nib = [current_sample['image'] for current_sample in data_meta['training']][\n",
    "                                       num_train:]\n",
    "            self.filenames_label_nib = [current_sample['label'] for current_sample in data_meta['training']][\n",
    "                                       num_train:]\n",
    "\n",
    "            self.filenames_image_npy = [os.path.join('.', 'imagesTrNP', f'{filename[11:-7]}.npy') for filename in\n",
    "                                        self.filenames_image_nib]\n",
    "            self.filenames_label_npy = [os.path.join('.', 'labelsTrNP', f'{filename[11:-7]}.npy') for filename in\n",
    "                                        self.filenames_label_nib]\n",
    "            # self.num_samples = int(np.ceil((1 - self.train_percentage) * num_samples))\n",
    "            self.num_samples = len(self.filenames_image_nib)\n",
    "        if (not len(self.filenames_image_nib) == len(self.filenames_label_nib)):\n",
    "            raise Exception('Inconsistent training image/label combination')\n",
    "        if len(self.filenames_image_nib) == 0:\n",
    "            raise Exception(f'Error reading {self.run_mode} images')\n",
    "        if len(self.filenames_label_nib) == 0:\n",
    "            raise Exception(f'Error reading {self.run_mode} labels')\n",
    "\n",
    "        elif self.run_mode == 'test':\n",
    "            # 'TODO' correct the train and test and inference variants\n",
    "            self.filenames_image_nib = [current_sample for current_sample in data_meta['test']]\n",
    "            if len(self.filenames_image_nib) == 0:\n",
    "                raise Exception(f'Error reading {self.run_mode} images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7474ee",
   "metadata": {},
   "source": [
    "### DeepMedic Model ###\n",
    "#### Adapted from: https://github.com/pykao/BraTS2018-tumor-segmentation/blob/master/models/deepmedic.py ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63be558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    '''\n",
    "        Adapted from: https://github.com/pykao/BraTS2018-tumor-segmentation/blob/master/models/deepmedic.py\n",
    "    '''\n",
    "\n",
    "    def __init__(self, inplanes, planes):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.inplanes = inplanes\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, 3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = nn.Conv3d(planes, planes, 3, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.relu(self.bn1(self.conv1(x)))\n",
    "        y = self.bn2(self.conv2(y))\n",
    "        x = x[:, :, 2:-2, 2:-2, 2:-2]\n",
    "        y[:, :self.inplanes] += x\n",
    "        y = self.relu(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "def conv3x3(inplanes, planes, ksize=3):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv3d(inplanes, planes, ksize, bias=False),\n",
    "        nn.BatchNorm3d(planes),\n",
    "        nn.ReLU(inplace=True))\n",
    "\n",
    "\n",
    "def repeat(x, n=3):\n",
    "    # nc333\n",
    "    b, c, h, w, t = x.shape\n",
    "    x = x.unsqueeze(5).unsqueeze(4).unsqueeze(3)\n",
    "    x = x.repeat(1, 1, 1, n, 1, n, 1, n)\n",
    "    return x.view(b, c, n * h, n * w, n * t)\n",
    "\n",
    "\n",
    "class DeepMedic(nn.Module):\n",
    "    '''\n",
    "        Adapted from: https://github.com/pykao/BraTS2018-tumor-segmentation/blob/master/models/deepmedic.py\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_channels=1, n1=30, n2=40, n3=50, m=150, up=True):\n",
    "        super(DeepMedic, self).__init__()\n",
    "        # n1, n2, n3 = 30, 40, 50\n",
    "        num_classes = 3\n",
    "        n = 2 * n3\n",
    "        self.branch1 = nn.Sequential(\n",
    "            conv3x3(input_channels, n1),\n",
    "            conv3x3(n1, n1),\n",
    "            ResBlock(n1, n2),\n",
    "            ResBlock(n2, n2),\n",
    "            ResBlock(n2, n3))\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv3x3(input_channels, n1),\n",
    "            conv3x3(n1, n1),\n",
    "            conv3x3(n1, n2),\n",
    "            conv3x3(n2, n2),\n",
    "            conv3x3(n2, n2),\n",
    "            conv3x3(n2, n2),\n",
    "            conv3x3(n2, n3),\n",
    "            conv3x3(n3, n3))\n",
    "\n",
    "        self.up3 = nn.Upsample(scale_factor=3, mode='trilinear', align_corners=False) if up else repeat\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            conv3x3(n, m, 1),\n",
    "            conv3x3(m, m, 1),\n",
    "            nn.Conv3d(m, num_classes, 1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1, x2 = inputs\n",
    "        x1 = self.branch1(x1)\n",
    "        x2 = self.branch2(x2)\n",
    "        x2 = self.up3(x2)\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af41f8f",
   "metadata": {},
   "source": [
    "### Generalized Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1fe8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedDiceLoss(nn.Module):\n",
    "    '''\n",
    "        Following the equation from https://arxiv.org/abs/1707.03237 page 3\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeneralizedDiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, im_pred, im_real):\n",
    "        if len(im_pred.shape) == 4:\n",
    "            im_pred = im_pred.unsqueeze(0)\n",
    "\n",
    "        if len(im_real.shape) == 4:\n",
    "            im_real = im_real.unsqueeze(0)\n",
    "\n",
    "        im_real = im_real.permute((1, 0, 2, 3, 4))\n",
    "        im_pred = im_pred.permute((1, 0, 2, 3, 4))\n",
    "\n",
    "        eps = 1e-12\n",
    "        sum_1 = torch.sum(im_real[0])\n",
    "        sum_2 = torch.sum(im_real[1])\n",
    "        sum_3 = torch.sum(im_real[2])\n",
    "\n",
    "        weight_1 = 1 / (sum_1 ** 2 + eps) if sum_1 > 0 else 1\n",
    "        weight_2 = 1 / (sum_2 ** 2 + eps) if sum_2 > 0 else 1\n",
    "        weight_3 = 1 / (sum_3 ** 2 + eps) if sum_3 > 0 else 1\n",
    "\n",
    "        numerator_1 = torch.sum(im_real[0] * im_pred[0]) * weight_1\n",
    "        numerator_2 = torch.sum(im_real[1] * im_pred[1]) * weight_2\n",
    "        numerator_3 = torch.sum(im_real[2] * im_pred[2]) * weight_3\n",
    "\n",
    "        numerator = numerator_1 + numerator_2 + numerator_3\n",
    "\n",
    "        denominator_1 = (torch.sum(im_real[0]) + torch.sum(im_pred[0])) * weight_1\n",
    "        denominator_2 = (torch.sum(im_real[1]) + torch.sum(im_pred[1])) * weight_2\n",
    "        denominator_3 = (torch.sum(im_real[2]) + torch.sum(im_pred[2])) * weight_3\n",
    "\n",
    "        denominator = denominator_1 + denominator_2 + denominator_3\n",
    "\n",
    "        dice_loss = 1 - ((2 * numerator) / (denominator + eps))\n",
    "\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Elastic Deformation Class"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class ElasticDeformation:\n",
    "    \"\"\"\n",
    "        Classs containing Elastic Deformation Transformation\n",
    "        Adapted from week 3 AML tutorial materials\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_controlpoints=None, sigma=None):\n",
    "\n",
    "        self.num_controlpoints = num_controlpoints\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # Random parameters if not defined\n",
    "        if self.sigma == None:\n",
    "            self.sigma = np.random.uniform(low=1, high=5)\n",
    "\n",
    "        if self.num_controlpoints == None:\n",
    "            self.num_controlpoints = int(np.random.uniform(low=1, high=6))\n",
    "\n",
    "    def create_elastic_deformation(self, image):\n",
    "        \"\"\"\n",
    "            We need to parameterise our b-spline transform\n",
    "            The transform will depend on such variables as image size and sigma\n",
    "            Sigma modulates the strength of the transformation\n",
    "            The number of control points controls the granularity of our transform\n",
    "        \"\"\"\n",
    "        # Create an instance of a SimpleITK image of the same size as our image\n",
    "        itkimg = sitk.GetImageFromArray(np.zeros(image.shape))\n",
    "\n",
    "        # This parameter is just a list with the number of control points per image dimensions\n",
    "        trans_from_domain_mesh_size = [self.num_controlpoints] * itkimg.GetDimension()\n",
    "\n",
    "        # We initialise the transform here: Passing the image size and the control point specifications\n",
    "        bspline_transformation = sitk.BSplineTransformInitializer(itkimg, trans_from_domain_mesh_size)\n",
    "\n",
    "        # Isolate the transform parameters: They will be all zero at this stage\n",
    "        params = np.asarray(bspline_transformation.GetParameters(), dtype=float)\n",
    "\n",
    "        # Let's initialise the transform by randomly initialising each parameter according to sigma\n",
    "        params = params + np.random.randn(params.shape[0]) * self.sigma\n",
    "\n",
    "        # Let's initialise the transform by randomly displacing each control point by a random distance (magnitude sigma)\n",
    "        bspline_transformation.SetParameters(tuple(params))\n",
    "\n",
    "        return bspline_transformation\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        # We need to choose an interpolation method for our transformed image, let's just go with b-spline\n",
    "        resampler = sitk.ResampleImageFilter()\n",
    "        resampler.SetInterpolator(sitk.sitkBSpline)\n",
    "\n",
    "        # Let's convert our image to an sitk image\n",
    "        sitk_image = sitk.GetImageFromArray(image)\n",
    "\n",
    "        # Specify the image to be transformed: This is the reference image\n",
    "        resampler.SetReferenceImage(sitk_image)\n",
    "        resampler.SetDefaultPixelValue(0)\n",
    "\n",
    "        # Initialise the transform\n",
    "        bspline_transform = self.create_elastic_deformation(image)\n",
    "\n",
    "        # Set the transform in the initialiser\n",
    "        resampler.SetTransform(bspline_transform)\n",
    "\n",
    "        # Carry out the resampling according to the transform and the resampling method\n",
    "        out_img_sitk = resampler.Execute(sitk_image)\n",
    "\n",
    "        # Convert the image back into a python array\n",
    "        out_img = sitk.GetArrayFromImage(out_img_sitk)\n",
    "\n",
    "        # We need to choose an interpolation method for our transformed image, let's just go with b-spline\n",
    "        resampler_label = sitk.ResampleImageFilter()\n",
    "        resampler_label.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "\n",
    "        # Let's convert our image to an sitk image\n",
    "        sitk_label = sitk.GetImageFromArray(label)\n",
    "\n",
    "        # Specify the image to be transformed: This is the reference image\n",
    "        resampler_label.SetReferenceImage(sitk_label)\n",
    "        resampler_label.SetDefaultPixelValue(0)\n",
    "\n",
    "        # Initialise the transform\n",
    "        bspline_transform = self.create_elastic_deformation(label)\n",
    "\n",
    "        # Set the transform in the initialiser\n",
    "        resampler_label.SetTransform(bspline_transform)\n",
    "\n",
    "        # Carry out the resampling according to the transform and the resampling method\n",
    "        out_label_sitk = resampler_label.Execute(sitk_label)\n",
    "\n",
    "        # Convert the image back into a python array\n",
    "        out_label = sitk.GetArrayFromImage(out_label_sitk)\n",
    "\n",
    "        return torch.tensor(out_img.reshape(image.shape), dtype=torch.float32), torch.tensor(out_label.reshape(image.shape), dtype=torch.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Affine Transformation Class ###"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class AffineTransformation:\n",
    "    \"\"\"\n",
    "        Classs containing Elastic Deformation Transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rotation=5, scale=(0.95, 1.05), shear=(0.01, 0.02), return_inverse=False, inverse_matrix=None):\n",
    "        self.rotation = rotation\n",
    "        self.scale = scale\n",
    "        self.shear = shear\n",
    "        self.return_inverse = return_inverse\n",
    "        self.inverse_matrix = inverse_matrix\n",
    "\n",
    "    def get_transformation_matrix(self):\n",
    "        # apply rotation on the z-axis\n",
    "        degree_rotation = torch.tensor(1, dtype=torch.float32).uniform_(-self.rotation, self.rotation)\n",
    "        degree_rotation = (degree_rotation * torch.pi) / 180\n",
    "\n",
    "        matrix_rotation = torch.zeros((1, 4, 4), dtype=torch.float32)\n",
    "        matrix_rotation[0, 0, 0] = torch.cos(degree_rotation)\n",
    "        matrix_rotation[0, 0, 1] = torch.sin(degree_rotation)\n",
    "        matrix_rotation[0, 1, 0] = -torch.sin(degree_rotation)\n",
    "        matrix_rotation[0, 1, 1] = torch.cos(degree_rotation)\n",
    "        matrix_rotation[0, 2, 2] = 1\n",
    "        matrix_rotation[0, 3, 3] = 1\n",
    "\n",
    "        # apply scaling on each dimension\n",
    "        matrix_scale = torch.zeros((1, 4, 4), dtype=torch.float32)\n",
    "        matrix_scale[0, 0, 0] = torch.tensor(1, dtype=torch.float32).uniform_(self.scale[0], self.scale[1])\n",
    "        matrix_scale[0, 1, 1] = torch.tensor(1, dtype=torch.float32).uniform_(self.scale[0], self.scale[1])\n",
    "        matrix_scale[0, 2, 2] = torch.tensor(1, dtype=torch.float32).uniform_(self.scale[0], self.scale[1])\n",
    "        matrix_scale[0, 3, 3] = 1\n",
    "        # print(matrix_scale.shape)\n",
    "\n",
    "        # shear\n",
    "        degree_shear = torch.tensor((\n",
    "            torch.tensor(1, dtype=torch.float32).uniform_(self.shear[0], self.shear[1]),\n",
    "            torch.tensor(1, dtype=torch.float32).uniform_(self.shear[0], self.shear[1])\n",
    "        ))\n",
    "\n",
    "        matrix_shear = torch.zeros((1, 4, 4), dtype=torch.float32)\n",
    "        matrix_shear[0, 0, 0] = 1\n",
    "        matrix_shear[0, 0, 1] = degree_shear[0]\n",
    "        matrix_shear[0, 1, 0] = degree_shear[1]\n",
    "        matrix_shear[0, 1, 1] = 1\n",
    "        matrix_shear[0, 2, 2] = 1\n",
    "        matrix_shear[0, 3, 3] = 1\n",
    "\n",
    "        # generate the combined affine transformation matrix\n",
    "        self.matrix_affine = torch.matmul(matrix_shear, torch.matmul(matrix_rotation, matrix_scale))\n",
    "\n",
    "        # generate the inverse transformation matrix\n",
    "        self.matrix_affine_inv = torch.inverse(self.matrix_affine)\n",
    "\n",
    "        # return to original coordinates\n",
    "        self.matrix_affine = self.matrix_affine[:, 0:3, :]\n",
    "        self.matrix_affine_inv = self.matrix_affine_inv[:, 0:3, :]\n",
    "\n",
    "    def __call__(self, image, label):\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(0).unsqueeze(0)\n",
    "        if len(label.shape) == 3:\n",
    "            label = label.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # obtain transformation matrix\n",
    "        self.get_transformation_matrix()\n",
    "\n",
    "        # define the affine grid and apply transformation on images and labels\n",
    "        if self.return_inverse:\n",
    "            grid_affine = F.affine_grid(self.matrix_affine_inv, image.shape, align_corners=False)\n",
    "            image_at = F.grid_sample(image.float(), grid_affine, padding_mode=\"border\", align_corners=False)\n",
    "            label_at = F.grid_sample(label.float(), grid_affine, mode='nearest', padding_mode=\"zeros\",\n",
    "                                        align_corners=False)\n",
    "        else:\n",
    "            grid_affine = F.affine_grid(self.matrix_affine, image.shape, align_corners=False)\n",
    "            image_at = F.grid_sample(image.float(), grid_affine, padding_mode=\"border\", align_corners=False)\n",
    "            label_at = F.grid_sample(label.float(), grid_affine, mode='nearest', padding_mode=\"zeros\",\n",
    "                                        align_corners=False)\n",
    "\n",
    "        return image_at.squeeze(0).squeeze(0), label_at.squeeze(0).squeeze(0), self.matrix_affine, self.matrix_affine_inv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a50481ab",
   "metadata": {},
   "source": [
    "### Container class for TRAIN/VAL/EVALUATE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3572a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConainer():\n",
    "    def __init__(self, params_model):\n",
    "        self.params_model = params_model\n",
    "\n",
    "    def __init_train_params(self):\n",
    "        self.run_mode = 'train'\n",
    "\n",
    "        self.loss_dict_train = {\n",
    "            'total': [],\n",
    "            'dice': [],\n",
    "            'mse': [],\n",
    "            'ce': [],\n",
    "            'dice_n_mse': [],\n",
    "            'dice_n_mse_n_ce': []\n",
    "        }\n",
    "\n",
    "        self.loss_dict_val = {\n",
    "            'total': [],\n",
    "            'dice': [],\n",
    "            'mse': [],\n",
    "            'ce': [],\n",
    "            'dice_n_mse': [],\n",
    "            'dice_n_mse_n_ce': []\n",
    "        }\n",
    "\n",
    "        self.loss_best_train = {\n",
    "            'total': np.inf,\n",
    "            'dice': np.inf,\n",
    "            'mse': np.inf,\n",
    "            'ce': np.inf,\n",
    "            'dice_n_mse': np.inf,\n",
    "            'dice_n_mse_n_ce': np.inf\n",
    "        }\n",
    "\n",
    "        self.loss_best_val = {\n",
    "            'total': np.inf,\n",
    "            'dice': np.inf,\n",
    "            'mse': np.inf,\n",
    "            'ce': np.inf,\n",
    "            'dice_n_mse': np.inf,\n",
    "            'dice_n_mse_n_ce': np.inf\n",
    "        }\n",
    "\n",
    "    def __init_inference_params(self):\n",
    "        self.run_mode = 'inference'\n",
    "\n",
    "    def __setup_logger(self):\n",
    "\n",
    "        if not self.params_train['resume_condition']:\n",
    "            reload(logging)\n",
    "\n",
    "        logging.basicConfig(filename=self.params_train['path_logger_full'], encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "    def __create_params_file(self):\n",
    "        params_dict = {\n",
    "            'params_model': self.params_model,\n",
    "            'params_train': self.params_train\n",
    "        }\n",
    "\n",
    "        params_dict = json.dumps(params_dict, indent=4, sort_keys=False)\n",
    "\n",
    "        with open(self.params_train['path_params_full'], 'w') as outfile:\n",
    "            outfile.write(params_dict)\n",
    "\n",
    "        self.__print(f'{\"*\" * 100}')\n",
    "        self.__print('\\t\\tTraining starting with params:')\n",
    "        self.__print(f'{\"*\" * 100}')\n",
    "        self.__print(f'{params_dict}')\n",
    "        self.__print(f'{\"*\" * 100}')\n",
    "\n",
    "    def __create_checkpoint_dir(self):\n",
    "        if self.params_train['resume_condition']:\n",
    "            self.params_train['dirname_checkpoint'] = self.params_train['resume_dir'][:11]\n",
    "            self.params_train['path_checkpoint_full'] = self.params_train['resume_dir']\n",
    "        else:\n",
    "            self.params_train['dirname_checkpoint'] = f'{self.params_model[\"experiment_name\"]}__' \\\n",
    "                                                      f'{self.params_model[\"init_timestamp\"]}__' \\\n",
    "                                                      f'{self.params_model[\"model_name\"]}__' \\\n",
    "                                                      f'{self.params_train[\"loss_name\"]}__' \\\n",
    "                                                      f'{self.params_train[\"optimizer_name\"]}__' \\\n",
    "                                                      f'lr_{self.params_train[\"learning_rate\"]}__' \\\n",
    "                                                      f'ep_{self.params_train[\"num_epochs\"]}'\n",
    "\n",
    "            self.params_train['path_checkpoint_full'] = os.path.join(self.params_train['path_checkpoint'],\n",
    "                                                                     self.params_train['dirname_checkpoint'])\n",
    "\n",
    "        self.params_train['path_params_full'] = os.path.join(self.params_train['path_checkpoint_full'],\n",
    "                                                             self.params_train['filename_params'])\n",
    "        self.params_train['path_logger_full'] = os.path.join(self.params_train['path_checkpoint_full'],\n",
    "                                                             self.params_train['filename_logger'])\n",
    "        os.makedirs(self.params_train['path_checkpoint_full'], exist_ok=True)\n",
    "\n",
    "    def train(self, params_train, transform_train=None):\n",
    "        self.params_train = params_train\n",
    "        self.transform_train = transform_train\n",
    "        self.__init_train_params()\n",
    "        self.__create_checkpoint_dir()\n",
    "        self.__create_params_file()\n",
    "        self.__setup_logger()\n",
    "        self.__fit_model()\n",
    "\n",
    "    def inference(self, params_inference):\n",
    "\n",
    "        self.params_inference = params_inference\n",
    "\n",
    "        self.__init_inference_params()\n",
    "        self.__run_inference()\n",
    "\n",
    "    def __set_device(self):\n",
    "        self.device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def __get_dataloaders(self, run_mode):\n",
    "        if run_mode == 'train':\n",
    "            self.dataset_train = DatasetHepatic(\n",
    "                run_mode='train',\n",
    "                transform_image=self.transform_train,\n",
    "                label_percentage=0.0001,\n",
    "                use_probabilistic=True,\n",
    "                patch_size_normal=self.params_model['patch_size_normal'],\n",
    "                patch_size_low=self.params_model['patch_size_low'],\n",
    "                patch_size_out=self.params_model['patch_size_out'],\n",
    "                patch_low_factor=self.params_model['patch_low_factor'],\n",
    "                create_numpy_dataset=self.params_model['create_numpy_dataset'],\n",
    "                dataset_variant=self.params_model['dataset_variant'],\n",
    "                batch_size_inner=self.params_train['batch_size_inner'],\n",
    "                train_percentage=self.params_train['train_percentage'],\n",
    "                use_elastic_deformation=self.params_train['use_elastic_deformation'],\n",
    "                user_affine_transformation=self.params_train['user_affine_transformation'],\n",
    "                num_controlpoints=self.params_train['num_controlpoints'],\n",
    "                sigma=self.params_train['sigma'],\n",
    "                rotation=self.params_train['rotation'],\n",
    "                scale=self.params_train['scale'],\n",
    "                shear=self.params_train['shear']\n",
    "            )\n",
    "\n",
    "            self.dataset_val = DatasetHepatic(\n",
    "                run_mode='val',\n",
    "                label_percentage=0.0001,\n",
    "                transform_image=None,\n",
    "                use_probabilistic=True,\n",
    "                patch_size_normal=self.params_model['patch_size_normal'],\n",
    "                patch_size_low=self.params_model['patch_size_low'],\n",
    "                patch_size_out=self.params_model['patch_size_out'],\n",
    "                patch_low_factor=self.params_model['patch_low_factor'],\n",
    "                create_numpy_dataset=self.params_model['create_numpy_dataset'],\n",
    "                dataset_variant=self.params_model['dataset_variant'],\n",
    "                batch_size_inner=self.params_train['batch_size_inner'],\n",
    "                train_percentage=self.params_train['train_percentage']\n",
    "            )\n",
    "\n",
    "            self.dataloader_train = DataLoader(\n",
    "                self.dataset_train,\n",
    "                batch_size=self.params_train['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=self.params_train['num_workers'],\n",
    "                pin_memory=self.params_train['pin_memory'],\n",
    "                prefetch_factor=self.params_train['prefetch_factor'],\n",
    "                persistent_workers=self.params_train['persistent_workers']\n",
    "            )\n",
    "\n",
    "            self.dataloader_val = DataLoader(\n",
    "                self.dataset_val,\n",
    "                batch_size=self.params_train['batch_size'],\n",
    "                shuffle=False,\n",
    "                num_workers=self.params_train['num_workers'],\n",
    "                pin_memory=self.params_train['pin_memory'],\n",
    "                prefetch_factor=self.params_train['prefetch_factor'],\n",
    "                persistent_workers=self.params_train['persistent_workers']\n",
    "            )\n",
    "\n",
    "        elif run_mode == 'inference':\n",
    "            self.dataset_inference = DatasetHepatic(\n",
    "                run_mode='inference',\n",
    "                transform_image=None,\n",
    "                label_percentage=0.0001,\n",
    "                use_probabilistic=True,\n",
    "                patch_size_normal=self.params_model['patch_size_normal'],\n",
    "                patch_size_low=self.params_model['patch_size_low'],\n",
    "                patch_size_out=self.params_model['patch_size_out'],\n",
    "                patch_low_factor=self.params_model['patch_low_factor'],\n",
    "                create_numpy_dataset=self.params_model['create_numpy_dataset'],\n",
    "                dataset_variant=self.params_model['dataset_variant']\n",
    "            )\n",
    "\n",
    "            self.dataloader_inference = DataLoader(\n",
    "                self.dataset_inference,\n",
    "                batch_size=self.params_inference['batch_size'],\n",
    "                shuffle=False,\n",
    "                num_workers=self.params_inference['num_workers'],\n",
    "                pin_memory=self.params_inference['pin_memory'],\n",
    "                prefetch_factor=self.params_inference['prefetch_factor'],\n",
    "                persistent_workers=self.params_inference['persistent_workers']\n",
    "            )\n",
    "\n",
    "    def __define_model(self):\n",
    "        if self.params_model['model_name'] == 'deep_medic':\n",
    "            self.model = DeepMedic().to(self.device)\n",
    "\n",
    "    def __define_criterions(self):\n",
    "        self.criterion_mse = nn.MSELoss()\n",
    "        self.criterion_dice = GeneralizedDiceLoss()\n",
    "        self.criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def __define_optimizr(self):\n",
    "        if self.params_train['optimizer_name'] == 'adam':\n",
    "            self.optimizer = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr=self.params_train['learning_rate'],\n",
    "                betas=(self.params_train['beta_1'], self.params_train['beta_2']),\n",
    "                amsgrad=self.params_train['use_amsgrad']\n",
    "            )\n",
    "\n",
    "        elif self.params_train['optimizer_name'] == 'sgd_w_momentum':\n",
    "            self.optimizer = optim.SGD(\n",
    "                self.model.parameters(),\n",
    "                lr=self.params_train['learning_rate'],\n",
    "                momentum=self.params_train['momentum']\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f'Invalid choice of optimizer:\\t{self.params_train[\"optimizer_name\"]}')\n",
    "\n",
    "    def __define_lr_scheduler(self):\n",
    "        if self.params_train['lr_scheduler_name'] == 'plateau':\n",
    "            self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer,\n",
    "                patience=self.params_train['patience_lr_scheduler'],\n",
    "                factor=self.params_train['factor_lr_scheduler'],\n",
    "                verbose=True)\n",
    "\n",
    "    def __put_to_device(self, device, tensors):\n",
    "        for index, tensor in enumerate(tensors):\n",
    "            tensors[index] = tensor.to(device)\n",
    "        return tensors\n",
    "\n",
    "    def __get_one_hot_labels(self, input, labels, squeeze_dim=None):\n",
    "        output = torch.zeros((input.shape[0], len(labels), input.shape[2], input.shape[3], input.shape[4]),\n",
    "                             dtype=input.dtype).to(input.device)\n",
    "\n",
    "        if not squeeze_dim is None:\n",
    "            for index, label in enumerate(labels):\n",
    "                output[:, index] = torch.where(input == label, 1, 0).squeeze(squeeze_dim)\n",
    "        else:\n",
    "            for index, label in enumerate(labels):\n",
    "                output[:, label] = torch.where(input == label, 1, 0)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __criterion_generalized_dice(self, im_real, im_pred):\n",
    "        '''\n",
    "            Following the equation from https://arxiv.org/abs/1707.03237 page 3\n",
    "        '''\n",
    "        weights = torch.autograd.Variable(3, dtype=torch.float64, requires_grad=True)\n",
    "        for index in range(3):\n",
    "            count = torch.tensor(torch.sum(torch.where(im_real == index, 1, 0)), dtype=torch.double, requires_grad=True)\n",
    "            # if none of the voxels are of the current category, set weight to 1\n",
    "            if count == 0:\n",
    "                weights[index] = torch.tensor(1, dtype=torch.double, requires_grad=True)\n",
    "            else:\n",
    "                weights[index] = 1 / count ** 2\n",
    "\n",
    "        numerator = torch.zeros(3, dtype=torch.double, requires_grad=True)\n",
    "        denominator = torch.zeros(3, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "        for index in range(3):\n",
    "            r_l_n = torch.where(im_real == index, 1, 0)\n",
    "            p_l_n = torch.where(im_pred == index, 1, 0)\n",
    "\n",
    "            # numerator\n",
    "            mult = r_l_n * p_l_n\n",
    "            numerator[index] = weights[index] * torch.sum(mult)\n",
    "\n",
    "            current_denominator = weights[index] * (torch.sum(r_l_n) + torch.sum(p_l_n))\n",
    "            denominator[index] = current_denominator\n",
    "\n",
    "        dice_loss = 1 - (2 * torch.sum(numerator) / torch.sum(denominator))\n",
    "\n",
    "        return dice_loss\n",
    "\n",
    "    def __save_model(self):\n",
    "        '''\n",
    "            Saves the model, best and the latest\n",
    "        '''\n",
    "        if self.params_train['save_condition']:\n",
    "\n",
    "            save_dict = {\n",
    "                'index_epoch': self.index_epoch + 1,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'lr_scheduler_state_dict': self.lr_scheduler.state_dict(),\n",
    "                'params_model': self.params_model,\n",
    "                'params_train': self.params_train,\n",
    "                'loss_dict_train': self.loss_dict_train,\n",
    "                'loss_dict_val': self.loss_dict_val,\n",
    "                'loss_best_train': self.loss_best_train,\n",
    "                'loss_best_val': self.loss_best_val\n",
    "            }\n",
    "\n",
    "            # save models at each epoch\n",
    "            if self.params_train['save_every_epoch']:\n",
    "                save_path = os.path.join(self.params_train['path_checkpoint_full'], f'{self.index_epoch + 1}.pth')\n",
    "                torch.save(save_dict, save_path)\n",
    "\n",
    "            # save the latest model\n",
    "            save_path = os.path.join(self.params_train['path_checkpoint_full'], f'latest.pth')\n",
    "            torch.save(save_dict, save_path)\n",
    "\n",
    "            if self.loss_dict_val['total'][-1] <= min(self.loss_dict_val['total']):\n",
    "                save_path = os.path.join(self.params_train['path_checkpoint_full'], f'best.pth')\n",
    "                torch.save(save_dict, save_path)\n",
    "                self.__print(f'{\"*\" * 10}\\tNew best model saved at:\\t{self.index_epoch + 1}\\t{\"*\" * 10}')\n",
    "\n",
    "    def __load_model(self):\n",
    "        '''\n",
    "            Loads the model\n",
    "        '''\n",
    "        if self.run_mode == 'train':\n",
    "            if self.params_train['resume_condition']:\n",
    "                filename_checkpoint = f'{self.params_train[\"resume_epoch\"]}.pth'\n",
    "                load_path = os.path.join(self.params_train['path_checkpoint'],\n",
    "                                         self.params_train['resume_dir'],\n",
    "                                         filename_checkpoint)\n",
    "\n",
    "                if not os.path.exists(load_path):\n",
    "                    raise FileNotFoundError(f'File {load_path} doesn\\'t exist')\n",
    "\n",
    "                checkpoint = torch.load(load_path)\n",
    "\n",
    "                self.index_epoch = checkpoint['index_epoch']\n",
    "                self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                self.lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "                self.params_model = checkpoint['params_model']\n",
    "                self.params_train = checkpoint['params_train']\n",
    "                self.loss_dict_train = checkpoint['loss_dict_train']\n",
    "                self.loss_dict_val = checkpoint['loss_dict_val']\n",
    "                self.loss_best_train = checkpoint['loss_best_train']\n",
    "                self.loss_best_val = checkpoint['loss_best_val']\n",
    "\n",
    "                self.__print(f'Model loaded from epoch:\\t{self.index_epoch}')\n",
    "                self.index_epoch += 1\n",
    "                self.start_epoch = self.index_epoch\n",
    "                self.__print(f'Resuming training from epoch:\\t{self.index_epoch}')\n",
    "\n",
    "        elif self.run_mode == 'inference':\n",
    "            filename_checkpoint = f'{self.params_inference[\"resume_epoch\"]}.pth'\n",
    "            load_path = os.path.join(self.params_inference['path_checkpoint'],\n",
    "                                     self.params_inference['resume_dir'],\n",
    "                                     filename_checkpoint)\n",
    "\n",
    "            if not os.path.exists(load_path):\n",
    "                raise FileNotFoundError(f'File {load_path} doesn\\'t exist')\n",
    "\n",
    "            checkpoint = torch.load(load_path)\n",
    "            self.index_epoch = checkpoint['index_epoch']\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            self.params_model = checkpoint['params_model']\n",
    "            self.index_epoch += 1\n",
    "            self.__print(f'Model loaded from epoch:\\t{self.index_epoch}')\n",
    "\n",
    "    def __early_stop(self):\n",
    "        '''\n",
    "            If early stopping condition meets, break training\n",
    "        '''\n",
    "        # train for at least self.params_train['min_epochs_to_train'] epochs\n",
    "        if self.index_epoch > self.params_train['min_epochs_to_train']:\n",
    "\n",
    "            # if the latest loss is lower than the best, continue training,\n",
    "            # otherwise check the last x losses loss_dict_val\n",
    "            # if self.loss_best_val['total'][-1] < min(self.loss_best_val['total']):\n",
    "            if self.loss_dict_val['total'][-1] < min(self.loss_dict_val['total']):\n",
    "                self.break_training_condition = False\n",
    "            else:\n",
    "                index_start = len(self.loss_dict_val['total']) - 1\n",
    "                index_stop = len(self.loss_dict_val['total']) - 1 - self.params_train['patience_early_stop']\n",
    "\n",
    "                # if any of the last x losses are greater than the best loss, increase counter\n",
    "                counter = 0\n",
    "                for index in range(index_start, index_stop, -1):\n",
    "                    if self.loss_dict_val['total'][index] > min(self.loss_dict_val['total']):\n",
    "                        counter += 1\n",
    "\n",
    "                # if counter equals the patience, break training\n",
    "                if counter >= self.params_train['patience_early_stop']:\n",
    "                    self.__print(f'Early stopping at epoch:\\t{self.index_epoch + 1}')\n",
    "                    self.break_training_condition = True\n",
    "\n",
    "    def __run_epoch(self, dataloader, run_mode):\n",
    "        '''\n",
    "            Runs one epoch of training and validation loops\n",
    "        '''\n",
    "        loss_list_total = []\n",
    "        loss_list_dice = []\n",
    "        loss_list_ce = []\n",
    "        loss_list_mse = []\n",
    "        loss_list_dice_n_mse = []\n",
    "        loss_list_dice_n_mse_n_ce = []\n",
    "\n",
    "        loss_total, loss_dice, loss_mse, loss_ce, loss_dice_n_mse, loss_dice_n_mse_n_ce = np.Inf, np.Inf, np.Inf, np.Inf, np.Inf, np.inf\n",
    "\n",
    "        for index_batch, batch in tqdm(enumerate(dataloader), leave=False, total=len(dataloader)):\n",
    "            if run_mode == 'train':\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "            else:\n",
    "                self.model.eval()\n",
    "\n",
    "            (image_patch_normal, image_patch_low_up, label_patch_out_real) = self.__put_to_device(self.device, batch)\n",
    "\n",
    "            if self.params_train['batch_size_inner'] > 1:\n",
    "                if len(image_patch_normal.shape) == 6:\n",
    "                    batch_size_stacked = image_patch_normal.shape[0] * image_patch_normal.shape[1]\n",
    "\n",
    "                    image_patch_normal = image_patch_normal.reshape(batch_size_stacked, image_patch_normal.shape[2],\n",
    "                                                                    image_patch_normal.shape[3],\n",
    "                                                                    image_patch_normal.shape[4],\n",
    "                                                                    image_patch_normal.shape[5])\n",
    "                    image_patch_low_up = image_patch_low_up.reshape(batch_size_stacked, image_patch_low_up.shape[2],\n",
    "                                                                    image_patch_low_up.shape[3],\n",
    "                                                                    image_patch_low_up.shape[4],\n",
    "                                                                    image_patch_low_up.shape[5])\n",
    "                    label_patch_out_real = label_patch_out_real.reshape(batch_size_stacked,\n",
    "                                                                        label_patch_out_real.shape[2],\n",
    "                                                                        label_patch_out_real.shape[3],\n",
    "                                                                        label_patch_out_real.shape[4],\n",
    "                                                                        label_patch_out_real.shape[5])\n",
    "\n",
    "                    image_patch_low = torch.zeros((image_patch_low_up.shape[0],\n",
    "                                                   self.params_model['patch_size_low'],\n",
    "                                                   self.params_model['patch_size_low'],\n",
    "                                                   self.params_model['patch_size_low'])).to(self.device)\n",
    "\n",
    "                else:\n",
    "                    image_patch_normal, image_patch_low_up, label_patch_out_real = image_patch_normal.squeeze(\n",
    "                        0), image_patch_low_up.squeeze(0), label_patch_out_real.squeeze(0)\n",
    "\n",
    "                image_patch_low = torch.zeros((image_patch_low_up.shape[0],\n",
    "                                               image_patch_low_up.shape[1],\n",
    "                                               self.params_model['patch_size_low'],\n",
    "                                               self.params_model['patch_size_low'],\n",
    "                                               self.params_model['patch_size_low'])).to(self.device)\n",
    "\n",
    "                for index, current_low_up in enumerate(image_patch_low_up):\n",
    "                    current_low = F.avg_pool3d(input=current_low_up, kernel_size=3, stride=None)\n",
    "                    image_patch_low[index] = copy.deepcopy(current_low.detach())\n",
    "\n",
    "            # forward pass\n",
    "            label_patch_out_pred = self.model.forward((image_patch_normal, image_patch_low))\n",
    "\n",
    "            # convert label_patch_out_real to one hot\n",
    "            label_patch_out_real_one_hot = self.__get_one_hot_labels(label_patch_out_real, labels=[0, 1, 2],\n",
    "                                                                     squeeze_dim=1)\n",
    "\n",
    "            # generalized dice loss_total # dice, mse, dice_n_mse\n",
    "            if self.params_train['loss_name'] == 'dice':\n",
    "                loss_dice = self.criterion_dice(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                                label_patch_out_real_one_hot.float())\n",
    "                loss_list_dice.append(loss_dice.item())\n",
    "                # print(f'loss_dice:\\t{loss_dice.item():.5f}')\n",
    "                loss_total = loss_dice\n",
    "            elif self.params_train['loss_name'] == 'mse':\n",
    "                loss_mse = self.criterion_mse(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                              label_patch_out_real_one_hot.float())\n",
    "                loss_list_mse.append(loss_mse.item())\n",
    "                loss_total = loss_mse\n",
    "            elif self.params_train['loss_name'] == 'ce':\n",
    "                loss_ce = self.criterion_ce(label_patch_out_pred.float(), label_patch_out_real.squeeze(1).long())\n",
    "                loss_list_ce.append(loss_ce.item())\n",
    "                loss_total = loss_ce\n",
    "            elif self.params_train['loss_name'] == 'dice_n_mse':\n",
    "                loss_dice = self.criterion_dice(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                                label_patch_out_real_one_hot.float())\n",
    "                loss_mse = self.criterion_mse(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                              label_patch_out_real_one_hot.float())\n",
    "                loss_dice_n_mse = loss_dice + loss_mse\n",
    "                loss_total = loss_dice_n_mse\n",
    "                loss_list_dice.append(loss_dice.item())\n",
    "                loss_list_mse.append(loss_mse.item())\n",
    "                loss_list_dice_n_mse.append(loss_dice_n_mse.item())\n",
    "            elif self.params_train['loss_name'] == 'dice_n_mse_n_ce':\n",
    "                loss_dice = self.criterion_dice(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                                label_patch_out_real_one_hot.float())\n",
    "                loss_mse = self.criterion_mse(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                              label_patch_out_real_one_hot.float())\n",
    "                loss_ce = self.criterion_ce(label_patch_out_pred.float(), label_patch_out_real.squeeze(1).long())\n",
    "\n",
    "                loss_dice_n_mse_n_ce = loss_dice + loss_mse + loss_ce\n",
    "\n",
    "                loss_total = loss_dice_n_mse_n_ce\n",
    "                loss_list_dice.append(loss_dice.item())\n",
    "                loss_list_mse.append(loss_mse.item())\n",
    "                loss_list_ce.append(loss_ce.item())\n",
    "                loss_list_dice_n_mse_n_ce.append(loss_dice_n_mse_n_ce.item())\n",
    "            else:\n",
    "                raise NotImplementedError(f'Invalid criterion selected:\\t{self.params_train[\"loss_name\"]}')\n",
    "\n",
    "            loss_list_total.append(loss_total)\n",
    "\n",
    "            if run_mode == 'train':\n",
    "                # calculate gradients and update weights\n",
    "                loss_total.backward()\n",
    "                self.optimizer.step()\n",
    "            sep = '\\t' if run_mode == 'train' else '\\t\\t'\n",
    "            # self.__print(f'\\tBatch:\\t[{index_batch + 1} / {len(dataloader)}]'\n",
    "            #                f'\\n\\t\\t{str(run_mode).upper()}{sep}-->\\t\\tLoss ({self.params_train[\"loss_name\"]}):\\t\\t{loss_total.item():.5f}')\n",
    "\n",
    "            # ###############################\n",
    "            # loss_mse = self.criterion_mse(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "            #                               label_patch_out_real_one_hot.float())\n",
    "            # loss_list_mse.append(loss_mse.item())\n",
    "            #\n",
    "            # print(f'Loss ({self.params_train[\"loss_name\"]}):\\t{loss_total.item():.5f}\\t\\tMSE:\\t{loss_mse.item()}')\n",
    "            # ###############################\n",
    "\n",
    "            # print(loss_dice.item())\n",
    "            # print(loss_mse.item())\n",
    "            # print(loss_list_dice_n_mse.item())\n",
    "            # break\n",
    "\n",
    "        # loss_dice = sum(loss_list_dice) / len(loss_list_dice)\n",
    "        # loss_mse = 0\n",
    "        # loss_dice_n_mse = 0\n",
    "\n",
    "        if len(loss_list_total) > 0:\n",
    "            loss_total = sum(loss_list_total) / len(loss_list_total)\n",
    "        if len(loss_list_dice) > 0:\n",
    "            loss_dice = sum(loss_list_dice) / len(loss_list_dice)\n",
    "        if len(loss_list_mse) > 0:\n",
    "            loss_mse = sum(loss_list_mse) / len(loss_list_mse)\n",
    "        if len(loss_list_ce) > 0:\n",
    "            loss_ce = sum(loss_list_ce) / len(loss_list_ce)\n",
    "        if len(loss_list_dice_n_mse) > 0:\n",
    "            loss_dice_n_mse = sum(loss_list_dice_n_mse) / len(loss_list_dice_n_mse)\n",
    "        if len(loss_list_dice_n_mse_n_ce) > 0:\n",
    "            loss_dice_n_mse_n_ce = sum(loss_list_dice_n_mse_n_ce) / len(loss_list_dice_n_mse_n_ce)\n",
    "\n",
    "        if run_mode == 'train':\n",
    "            self.loss_dict_train['total'].append(loss_total.item())\n",
    "            self.loss_dict_train['dice'].append(loss_dice)\n",
    "            self.loss_dict_train['mse'].append(loss_mse)\n",
    "            self.loss_dict_train['ce'].append(loss_ce)\n",
    "            self.loss_dict_train['dice_n_mse'].append(loss_dice_n_mse)\n",
    "            self.loss_dict_train['dice_n_mse_n_ce'].append(loss_dice_n_mse_n_ce)\n",
    "\n",
    "        elif run_mode == 'val':\n",
    "            self.loss_dict_val['total'].append(loss_total.item())\n",
    "            self.loss_dict_val['dice'].append(loss_dice)\n",
    "            self.loss_dict_val['mse'].append(loss_mse)\n",
    "            self.loss_dict_val['ce'].append(loss_ce)\n",
    "            self.loss_dict_val['dice_n_mse'].append(loss_dice_n_mse)\n",
    "            self.loss_dict_val['dice_n_mse_n_ce'].append(loss_dice_n_mse_n_ce)\n",
    "\n",
    "    def __update_best_losses(self):\n",
    "        '''\n",
    "            Updates the best loss found so far\n",
    "        '''\n",
    "        self.found_best_loss_flag = False\n",
    "\n",
    "        for (key, value) in self.loss_dict_train.items():\n",
    "            if self.loss_dict_train[key][-1] < min(self.loss_dict_train[key]):\n",
    "                self.loss_best_train[key] = self.loss_dict_train[key][-1]\n",
    "                # if self.params_train['loss_name'] == key:\n",
    "                #     self.found_best_loss_flag = True\n",
    "\n",
    "        for (key, value) in self.loss_dict_val.items():\n",
    "            if self.loss_dict_val[key][-1] < min(self.loss_dict_val[key]):\n",
    "                self.loss_best_val[key] = self.loss_dict_val[key][-1]\n",
    "                if self.params_train['loss_name'] == key:\n",
    "                    self.found_best_loss_flag = True\n",
    "\n",
    "    def __print(self, message):\n",
    "        print(message)\n",
    "        logging.debug(message)\n",
    "        logging.debug('working')\n",
    "\n",
    "    def __fit_model(self):\n",
    "        '''\n",
    "            Trains and validates a model given hyperparameters, model and optimizer name, dataloaders, num_epochs\n",
    "        '''\n",
    "\n",
    "        # set up dataloaders, model, criterions, optimizers, schedulers\n",
    "        self.__set_device()\n",
    "        self.__get_dataloaders('train')\n",
    "        self.__define_model()\n",
    "        self.__define_criterions()\n",
    "        self.__define_optimizr()\n",
    "        self.__define_lr_scheduler()\n",
    "\n",
    "        # variables to keep track of training progress\n",
    "        self.start_epoch = 0\n",
    "        self.break_training_condition = False\n",
    "        self.end_epoch = self.params_train['num_epochs']\n",
    "\n",
    "        self.__load_model()\n",
    "\n",
    "        for index_epoch in range(self.start_epoch, self.end_epoch):\n",
    "            time_start = time.time()\n",
    "            self.index_epoch = index_epoch\n",
    "\n",
    "            # train\n",
    "            self.__run_epoch(\n",
    "                dataloader=self.dataloader_train,\n",
    "                run_mode='train'\n",
    "            )\n",
    "\n",
    "            # validation\n",
    "            with torch.no_grad():\n",
    "                self.__run_epoch(\n",
    "                    dataloader=self.dataloader_val,\n",
    "                    run_mode='val'\n",
    "                )\n",
    "\n",
    "            # choose which loss to put into the scheduler\n",
    "            self.lr_scheduler.step(self.loss_dict_val['total'][-1])\n",
    "\n",
    "            duration = time.time() - time_start\n",
    "\n",
    "            self.__print(f'\\n{\"-\" * 100}'\n",
    "                         f'\\nEpoch:\\t[{index_epoch + 1} / {self.end_epoch}]\\t\\t'\n",
    "                         f'Time:\\t{duration:.2f} s'\n",
    "                         f'\\n\\tTRAIN\\t\\t-->\\t\\tLoss Total:\\t\\t{self.loss_dict_train[\"total\"][-1]:.5f}'\n",
    "                         f'\\n\\tVAL\\t\\t\\t-->\\t\\tLoss Total:\\t\\t{self.loss_dict_val[\"total\"][-1]:.5f}'\n",
    "                         f'\\t\\tBest:\\t{min(self.loss_dict_val[\"total\"]):.5f}'\n",
    "                         f'\\n{\"-\" * 100}\\n')\n",
    "\n",
    "            self.__update_best_losses()\n",
    "            self.__save_model()\n",
    "            self.__early_stop()\n",
    "            # self.__early_stopper()\n",
    "\n",
    "            if self.break_training_condition:\n",
    "                break\n",
    "\n",
    "    def __run_inference(self):\n",
    "        '''\n",
    "            Run 3D inference on the validation set. Generates a 3D volume of predicted labels with same shape as the original one\n",
    "        '''\n",
    "        self.__set_device()\n",
    "        self.__get_dataloaders('inference')\n",
    "        self.__define_model()\n",
    "        self.__define_criterions()\n",
    "        self.__load_model()\n",
    "\n",
    "        self.__print(f'{\"*\" * 100}')\n",
    "        self.__print('\\t\\tInference starting with params:')\n",
    "        self.__print(f'{\"*\" * 100}')\n",
    "        params_dict = {\n",
    "            'params_model': self.params_model,\n",
    "            'params_inference': self.params_inference\n",
    "        }\n",
    "        params_dict = json.dumps(params_dict, indent=4, sort_keys=False)\n",
    "        self.__print(f'{params_dict}')\n",
    "        self.__print(f'{\"*\" * 100}')\n",
    "\n",
    "        for index_batch, batch in enumerate(self.dataloader_inference):\n",
    "            # save only the last 30 samples to match with other group members\n",
    "            if index_batch < 30:\n",
    "                continue\n",
    "            images, labels_real, index_filename = batch\n",
    "            (images, labels_real) = self.__put_to_device(self.device, [images, labels_real])\n",
    "\n",
    "            labels_pred, labels_pred_probabilistic, loss_dice, loss_mse = self.__stride_depth_and_inference(\n",
    "                images_real=images,\n",
    "                labels_real=labels_real\n",
    "            )\n",
    "            # assuming the batch size == 1\n",
    "            index_filename = index_filename[0]\n",
    "            self.__print(\n",
    "                f'{index_batch + 1}: \\t{index_batch}.npy\\tLoss DICE:\\t{loss_dice:.5f}\\tLoss MSE:\\t{loss_mse:.5f}')\n",
    "\n",
    "            labels_real, labels_pred, labels_pred_probabilistic = labels_real.cpu().detach().numpy(), labels_pred.cpu().detach().numpy(), labels_pred_probabilistic.cpu().detach().numpy()\n",
    "\n",
    "            predictions_path = os.path.join('.', 'Ensamble', 'Abhijit')\n",
    "            labels_path = os.path.join('.', 'labels_true')\n",
    "            os.makedirs(predictions_path, exist_ok=True)\n",
    "            os.makedirs(labels_path, exist_ok=True)\n",
    "            # saves the probabilistic outputs (bs x 3 x h x w x d)\n",
    "\n",
    "            # save predictions\n",
    "            np.save(os.path.join(predictions_path, f'{index_batch}.npy'), labels_pred_probabilistic,\n",
    "                    allow_pickle=True)\n",
    "\n",
    "            # save the actual labels\n",
    "            np.save(os.path.join(labels_path, f'{index_batch}.npy'), labels_real,\n",
    "                    allow_pickle=True)\n",
    "\n",
    "    def __stride_depth_and_inference(self, images_real, labels_real):\n",
    "        self.model.eval()\n",
    "        patch_size_normal = self.params_model['patch_size_normal']\n",
    "        patch_size_low = self.params_model['patch_size_low']\n",
    "        patch_size_out = self.params_model['patch_size_out']\n",
    "        patch_low_factor = self.params_model['patch_low_factor']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_list_dice = []\n",
    "            loss_list_mse = []\n",
    "\n",
    "            device = images_real.device\n",
    "            batch_size, height, width, depth = images_real.shape\n",
    "\n",
    "            # --------- loop through the whole image volume\n",
    "            patch_size_low_up = patch_size_low * patch_low_factor\n",
    "\n",
    "            patch_half_normal = (patch_size_normal - 1) // 2\n",
    "            patch_half_low = (patch_size_low - 1) // 2\n",
    "            patch_half_low_up = (patch_size_low_up - 1) // 2\n",
    "            patch_half_out = (patch_size_out - 1) // 2\n",
    "\n",
    "            height_new = height + patch_size_low_up\n",
    "            width_new = width + patch_size_low_up\n",
    "            depth_new = depth + patch_size_low_up\n",
    "\n",
    "            # create a placeholder for the padded image\n",
    "            images_padded = torch.zeros((batch_size, height_new, width_new, depth_new), dtype=torch.float32).to(device)\n",
    "            labels_real_padded = torch.zeros((batch_size, height_new, width_new, depth_new), dtype=torch.float32).to(\n",
    "                device)\n",
    "\n",
    "            # labels_padded = torch.zeros((batch_size, height_new, width_new, depth_new), dtype=torch.float32).to(device)\n",
    "            # print(f'images_real.shape:\\t{images_real.shape}')\n",
    "            # print(f'images_padded.shape:\\t{images_padded.shape}')\n",
    "\n",
    "            # copy the original image to the placeholder\n",
    "            images_padded[\n",
    "            :,\n",
    "            patch_half_low_up: height + patch_half_low_up,\n",
    "            patch_half_low_up: width + patch_half_low_up,\n",
    "            patch_half_low_up: depth + patch_half_low_up\n",
    "            ] = copy.deepcopy(images_real).to(device)\n",
    "\n",
    "            labels_real_padded[\n",
    "            :,\n",
    "            patch_half_low_up: height + patch_half_low_up,\n",
    "            patch_half_low_up: width + patch_half_low_up,\n",
    "            patch_half_low_up: depth + patch_half_low_up\n",
    "            ] = copy.deepcopy(labels_real).to(device)\n",
    "\n",
    "            # print(f'{patch_half_low_up} -> {height + patch_half_low_up}')\n",
    "            # print(f'{patch_half_low_up} -> {width + patch_half_low_up}')\n",
    "            # print(f'{patch_half_low_up} -> {depth + patch_half_low_up}')\n",
    "\n",
    "            # placeholder to store the inferred/reconstructed image labels\n",
    "            labels_pred_whole_image = torch.zeros_like(images_real).to(device)\n",
    "            labels_pred_whole_image_probabilistic = torch.zeros((batch_size, 3, height, width, depth),\n",
    "                                                                dtype=torch.float32).to(device)\n",
    "            # print(f'labels_pred_whole_image.shape:\\t{labels_pred_whole_image.shape}')\n",
    "\n",
    "            # indices of the original image\n",
    "            h_start_orig = 0\n",
    "            h_end_orig = h_start_orig + patch_size_out\n",
    "\n",
    "            for index_h in tqdm(range(patch_half_low_up, height_new - patch_half_low_up, patch_size_out),\n",
    "                                leave=False):\n",
    "                # print(index_h)\n",
    "                h_start_normal = index_h - patch_half_normal\n",
    "                h_end_normal = index_h + patch_half_normal + 1\n",
    "\n",
    "                h_start_low_up = index_h - patch_half_low_up\n",
    "                h_end_low_up = index_h + patch_half_low_up + 1\n",
    "\n",
    "                h_start_out = index_h - patch_half_out\n",
    "                h_end_out = index_h + patch_half_out + 1\n",
    "\n",
    "                # if the starting index of the out height > padded height; break\n",
    "                if h_end_out > height_new:\n",
    "                    break\n",
    "\n",
    "                w_start_orig = 0\n",
    "                w_end_orig = w_start_orig + patch_size_out\n",
    "\n",
    "                for index_w in range(patch_half_low_up, width_new - patch_half_low_up, patch_size_out):\n",
    "\n",
    "                    w_start_normal = index_w - patch_half_normal\n",
    "                    w_end_normal = index_w + patch_half_normal + 1\n",
    "\n",
    "                    w_start_low_up = index_w - patch_half_low_up\n",
    "                    w_end_low_up = index_w + patch_half_low_up + 1\n",
    "\n",
    "                    w_start_out = index_w - patch_half_out\n",
    "                    w_end_out = index_w + patch_half_out + 1\n",
    "\n",
    "                    if w_end_out > width_new:\n",
    "                        break\n",
    "\n",
    "                    d_start_orig = 0\n",
    "                    d_end_orig = d_start_orig + patch_size_out\n",
    "\n",
    "                    for index_d in range(patch_half_low_up, depth_new - patch_half_low_up, patch_size_out):\n",
    "\n",
    "                        d_start_normal = index_d - patch_half_normal\n",
    "                        d_end_normal = index_d + patch_half_normal + 1\n",
    "\n",
    "                        d_start_low_up = index_d - patch_half_low_up\n",
    "                        d_end_low_up = index_d + patch_half_low_up + 1\n",
    "\n",
    "                        d_start_out = index_d - patch_half_out\n",
    "                        d_end_out = index_d + patch_half_out + 1\n",
    "\n",
    "                        if d_end_out > depth_new:\n",
    "                            break\n",
    "\n",
    "                        # extract the current patch of the expanded image\n",
    "                        image_patch_normal = images_padded[\n",
    "                                             :,\n",
    "                                             h_start_normal: h_end_normal,\n",
    "                                             w_start_normal: w_end_normal,\n",
    "                                             d_start_normal: d_end_normal\n",
    "                                             ]\n",
    "                        # print('\\nNormal')\n",
    "                        # print(f'{h_start_normal} -> {h_end_normal}')\n",
    "                        # print(f'{w_start_normal} -> {w_end_normal}')\n",
    "                        # print(f'{d_start_normal} -> {d_end_normal}')\n",
    "\n",
    "                        image_patch_low_up = images_padded[\n",
    "                                             :,\n",
    "                                             h_start_low_up: h_end_low_up,\n",
    "                                             w_start_low_up: w_end_low_up,\n",
    "                                             d_start_low_up: d_end_low_up\n",
    "                                             ]\n",
    "\n",
    "                        # print('\\nlow_up')\n",
    "                        # print(f'{h_start_low_up} -> {h_end_low_up}')\n",
    "                        # print(f'{w_start_low_up} -> {w_end_low_up}')\n",
    "                        # print(f'{d_start_low_up} -> {d_end_low_up}')\n",
    "\n",
    "                        # extract the current output patch of the expanded label\n",
    "                        label_patch_out_real = labels_real_padded[\n",
    "                                               :,\n",
    "                                               h_start_out: h_end_out,\n",
    "                                               w_start_out: w_end_out,\n",
    "                                               d_start_out: d_end_out\n",
    "                                               ]\n",
    "                        # print('\\nout')\n",
    "                        # print(f'{h_start_out} -> {h_end_out}')\n",
    "                        # print(f'{w_start_out} -> {w_end_out}')\n",
    "                        # print(f'{d_start_out} -> {d_end_out}')\n",
    "\n",
    "                        # if d_start_out == 42:\n",
    "                        #     print('sssss')\n",
    "\n",
    "                        if not (label_patch_out_real.shape[1] * label_patch_out_real.shape[2] *\n",
    "                                label_patch_out_real.shape[3] > 0):\n",
    "                            # print('here')\n",
    "                            continue\n",
    "\n",
    "                        # pad uneven images (image patch normal)\n",
    "                        image_patch_normal_temp = torch.zeros(\n",
    "                            (batch_size, patch_size_normal, patch_size_normal, patch_size_normal)).to(device)\n",
    "                        image_patch_normal_temp[:, :image_patch_normal.shape[1], :image_patch_normal.shape[2],\n",
    "                        :image_patch_normal.shape[3]] = image_patch_normal\n",
    "                        image_patch_normal = image_patch_normal_temp\n",
    "\n",
    "                        # pad uneven images (image patch low_up)\n",
    "                        image_patch_low_up_temp = torch.zeros(\n",
    "                            (batch_size, patch_size_low_up, patch_size_low_up, patch_size_low_up)).to(device)\n",
    "                        image_patch_low_up_temp[:, :image_patch_low_up.shape[1], :image_patch_low_up.shape[2],\n",
    "                        :image_patch_low_up.shape[3]] = image_patch_low_up\n",
    "                        image_patch_low_up = image_patch_low_up_temp\n",
    "\n",
    "                        # resize (downsample) image_patch_low\n",
    "                        image_patch_low = F.avg_pool3d(input=image_patch_low_up, kernel_size=3, stride=None)\n",
    "\n",
    "                        # perform forward pass\n",
    "                        label_patch_out_pred = self.model.forward(\n",
    "                            (image_patch_normal.unsqueeze(0), image_patch_low.unsqueeze(0)))\n",
    "\n",
    "                        # print(label_patch_out_real.shape)\n",
    "                        # clip extra parts\n",
    "                        if label_patch_out_real.shape[1] < patch_size_out:\n",
    "                            label_patch_out_pred = label_patch_out_pred[:, :, :label_patch_out_real.shape[1], :, :]\n",
    "\n",
    "                        if label_patch_out_real.shape[2] < patch_size_out:\n",
    "                            label_patch_out_pred = label_patch_out_pred[:, :, :, :label_patch_out_real.shape[2], :]\n",
    "\n",
    "                        if label_patch_out_real.shape[3] < patch_size_out:\n",
    "                            label_patch_out_pred = label_patch_out_pred[:, :, :, :, :label_patch_out_real.shape[3]]\n",
    "\n",
    "                        # # remove any dimensions with 0 elements\n",
    "                        # if (label_patch_out_pred.shape[2] == 0) or (label_patch_out_pred.shape[3] == 0) or (label_patch_out_pred.shape[4] == 0) or (\n",
    "                        #         label_patch_out_real.shape[2] == 0) or (label_patch_out_real.shape[3] == 0) or (\n",
    "                        #         label_patch_out_real.shape[4] == 0):\n",
    "                        #     break\n",
    "\n",
    "                        # print(label_patch_out_pred.shape)\n",
    "                        # convert label_patch_out_real to one hot\n",
    "                        label_patch_out_real_one_hot = torch.zeros_like(label_patch_out_pred).to(device)\n",
    "                        # print(label_patch_out_real_one_hot.shape)\n",
    "                        label_patch_out_real_one_hot[:, 0] = torch.where(label_patch_out_real == 0, 1, 0)\n",
    "                        label_patch_out_real_one_hot[:, 1] = torch.where(label_patch_out_real == 1, 1, 0)\n",
    "                        label_patch_out_real_one_hot[:, 2] = torch.where(label_patch_out_real == 2, 1, 0)\n",
    "\n",
    "                        # cross-entropy loss_dice\n",
    "                        loss_dice = self.criterion_dice(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                                        label_patch_out_real_one_hot.float())\n",
    "                        loss_mse = self.criterion_mse(F.softmax(label_patch_out_pred.float(), dim=1),\n",
    "                                                      label_patch_out_real_one_hot.float())\n",
    "                        # print(loss_mse.item())\n",
    "                        loss_list_dice.append(loss_dice)\n",
    "                        loss_list_mse.append(loss_mse)\n",
    "                        # print(loss_dice)\n",
    "\n",
    "                        label_patch_out_pred_double = torch.argmax(label_patch_out_pred.detach(), dim=1)\n",
    "                        label_patch_out_pred_double_temp = torch.zeros(batch_size, patch_size_out, patch_size_out,\n",
    "                                                                       patch_size_out).to(device)\n",
    "                        label_patch_out_pred_double_temp[:, :label_patch_out_pred_double.shape[1],\n",
    "                        :label_patch_out_pred_double.shape[2],\n",
    "                        :label_patch_out_pred_double.shape[3]] = label_patch_out_pred_double\n",
    "                        label_patch_out_pred_double = label_patch_out_pred_double_temp\n",
    "\n",
    "                        bs, h, w, d = labels_pred_whole_image[:, h_start_orig: h_end_orig,\n",
    "                                      w_start_orig: w_end_orig,\n",
    "                                      d_start_orig: d_end_orig].shape\n",
    "\n",
    "                        # save the pixel wise predictions\n",
    "                        labels_pred_whole_image[:, h_start_orig: h_end_orig, w_start_orig: w_end_orig,\n",
    "                        d_start_orig: d_end_orig] = label_patch_out_pred_double[:, :h, :w, :d].detach()\n",
    "\n",
    "                        # save the probabilistic predictions\n",
    "                        labels_pred_whole_image_probabilistic[:, :, h_start_orig: h_end_orig, w_start_orig: w_end_orig,\n",
    "                        d_start_orig: d_end_orig] = label_patch_out_pred[:, :, :h, :w, :d].detach()\n",
    "\n",
    "                        d_start_orig = d_start_orig + patch_size_out\n",
    "                        d_end_orig = d_end_orig + patch_size_out\n",
    "\n",
    "                    w_start_orig = w_start_orig + patch_size_out\n",
    "                    w_end_orig = w_end_orig + patch_size_out\n",
    "\n",
    "                h_start_orig = h_start_orig + patch_size_out\n",
    "                h_end_orig = h_end_orig + patch_size_out\n",
    "\n",
    "                loss_dice = sum(loss_list_dice) / (len(loss_list_dice) + 1e-9)\n",
    "                loss_mse = sum(loss_list_mse) / (len(loss_list_mse) + 1e-9)\n",
    "\n",
    "        return labels_pred_whole_image, labels_pred_whole_image_probabilistic, loss_dice, loss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8207e",
   "metadata": {},
   "source": [
    "# STEP 1 #\n",
    "\n",
    "Write a segmentation algorithm pipeline. Train on the training set, defined as a proportion of the data in imagesTr, and validate the algorithm performance on the remaining images. Do not use any auwgmentation for now. Use any choice of optimiser.\n",
    "Describe how the algorithm was trained, and what were the final results using standard image segmentation validation metrics such as Dice Score or Hausdorff Distance.\n",
    "\n",
    "Answer Marks:\n",
    "\n",
    "[10] Working algorithmic implementation\n",
    "\n",
    "[ 3] Comments on the code\n",
    "\n",
    "[ 9] Description of the training process\n",
    "\n",
    "[ 8] Validation presentation and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d077bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\t\tTraining starting with params:\n",
      "****************************************************************************************************\n",
      "{\n",
      "    \"params_model\": {\n",
      "        \"experiment_name\": \"step_1\",\n",
      "        \"model_name\": \"deep_medic\",\n",
      "        \"patch_size_normal\": 25,\n",
      "        \"patch_size_low\": 19,\n",
      "        \"patch_size_out\": 9,\n",
      "        \"patch_low_factor\": 3,\n",
      "        \"run_mode\": null,\n",
      "        \"dataset_variant\": \"npy\",\n",
      "        \"create_numpy_dataset\": false,\n",
      "        \"init_timestamp\": \"15-08-38__05-04-2022\"\n",
      "    },\n",
      "    \"params_train\": {\n",
      "        \"optimizer_name\": \"adam\",\n",
      "        \"loss_name\": \"dice\",\n",
      "        \"beta_1\": 0.9,\n",
      "        \"beta_2\": 0.999,\n",
      "        \"momentum\": 0.9,\n",
      "        \"use_amsgrad\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"lr_scheduler_name\": \"plateau\",\n",
      "        \"patience_lr_scheduler\": 2,\n",
      "        \"factor_lr_scheduler\": 0.1,\n",
      "        \"early_stop_condition\": true,\n",
      "        \"patience_early_stop\": 5,\n",
      "        \"early_stop_patience_counter\": 0,\n",
      "        \"min_epochs_to_train\": 10,\n",
      "        \"num_epochs\": 100,\n",
      "        \"save_every_epoch\": true,\n",
      "        \"save_condition\": true,\n",
      "        \"resume_condition\": false,\n",
      "        \"resume_dir\": \"14-10-08__01-04-2022__deep_medic__dice__adam__lr_0.0001__ep_50\",\n",
      "        \"resume_epoch\": \"latest\",\n",
      "        \"batch_size\": 8,\n",
      "        \"batch_size_inner\": 16,\n",
      "        \"train_percentage\": 0.8,\n",
      "        \"num_workers\": 8,\n",
      "        \"pin_memory\": true,\n",
      "        \"prefetch_factor\": 2,\n",
      "        \"persistent_workers\": true,\n",
      "        \"path_checkpoint\": \"./checkpoints\",\n",
      "        \"path_checkpoint_full\": \"./checkpoints/step_1__15-08-38__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"dirname_checkpoint\": \"step_1__15-08-38__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"filename_params\": \"params.json\",\n",
      "        \"filename_logger\": \"logger.txt\",\n",
      "        \"path_params_full\": \"./checkpoints/step_1__15-08-38__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100/params.json\",\n",
      "        \"path_logger_full\": \"./checkpoints/step_1__15-08-38__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100/logger.txt\",\n",
      "        \"use_elastic_deformation\": false,\n",
      "        \"user_affine_transformation\": false,\n",
      "        \"num_controlpoints\": 20,\n",
      "        \"sigma\": 5,\n",
      "        \"rotation\": 10,\n",
      "        \"scale\": [\n",
      "            0.9,\n",
      "            1.1\n",
      "        ],\n",
      "        \"shear\": [\n",
      "            0.01,\n",
      "            0.02\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[1 / 100]\t\tTime:\t433.39 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.54777\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.43942\t\tBest:\t0.43942\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t1\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[2 / 100]\t\tTime:\t436.38 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.44131\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.37737\t\tBest:\t0.37737\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t2\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[3 / 100]\t\tTime:\t451.40 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.40754\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.40235\t\tBest:\t0.37737\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[4 / 100]\t\tTime:\t436.17 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.40538\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.35128\t\tBest:\t0.35128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t4\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[5 / 100]\t\tTime:\t446.50 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.35426\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.33229\t\tBest:\t0.33229\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t5\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[6 / 100]\t\tTime:\t442.12 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.38096\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.33042\t\tBest:\t0.33042\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t6\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[7 / 100]\t\tTime:\t463.92 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.34211\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.30315\t\tBest:\t0.30315\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t7\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[8 / 100]\t\tTime:\t434.22 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.33924\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.33920\t\tBest:\t0.30315\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[9 / 100]\t\tTime:\t438.07 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.32699\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.28170\t\tBest:\t0.28170\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t9\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[10 / 100]\t\tTime:\t476.14 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.32182\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.27461\t\tBest:\t0.27461\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t10\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[11 / 100]\t\tTime:\t448.98 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.30836\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.28935\t\tBest:\t0.27461\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[12 / 100]\t\tTime:\t466.97 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.31678\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25937\t\tBest:\t0.25937\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t12\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[13 / 100]\t\tTime:\t458.61 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.29453\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.28866\t\tBest:\t0.25937\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[14 / 100]\t\tTime:\t445.32 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.32577\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25669\t\tBest:\t0.25669\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t14\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[15 / 100]\t\tTime:\t448.86 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.27631\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.30026\t\tBest:\t0.25669\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[16 / 100]\t\tTime:\t444.38 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28012\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.27703\t\tBest:\t0.25669\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[17 / 100]\t\tTime:\t436.28 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.29427\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.24486\t\tBest:\t0.24486\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t17\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[18 / 100]\t\tTime:\t435.14 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28402\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.29928\t\tBest:\t0.24486\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[19 / 100]\t\tTime:\t484.84 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.29069\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25004\t\tBest:\t0.24486\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[20 / 100]\t\tTime:\t460.05 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26589\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23152\t\tBest:\t0.23152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t20\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[21 / 100]\t\tTime:\t433.85 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25900\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25875\t\tBest:\t0.23152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[22 / 100]\t\tTime:\t438.11 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26296\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25247\t\tBest:\t0.23152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[23 / 100]\t\tTime:\t457.75 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26025\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23113\t\tBest:\t0.23113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t23\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[24 / 100]\t\tTime:\t470.61 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25454\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23959\t\tBest:\t0.23113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[25 / 100]\t\tTime:\t455.79 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.27519\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.24448\t\tBest:\t0.23113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00026: reducing learning rate of group 0 to 2.0000e-05.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[26 / 100]\t\tTime:\t486.99 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26176\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.24437\t\tBest:\t0.23113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[27 / 100]\t\tTime:\t428.83 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24735\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23922\t\tBest:\t0.23113\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[28 / 100]\t\tTime:\t453.69 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.23339\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.21903\t\tBest:\t0.21903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t28\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[29 / 100]\t\tTime:\t440.86 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24715\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22200\t\tBest:\t0.21903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[30 / 100]\t\tTime:\t467.61 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25004\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22617\t\tBest:\t0.21903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00031: reducing learning rate of group 0 to 2.0000e-06.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[31 / 100]\t\tTime:\t435.11 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.22246\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23199\t\tBest:\t0.21903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[32 / 100]\t\tTime:\t477.63 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.23143\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23155\t\tBest:\t0.21903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[33 / 100]\t\tTime:\t471.26 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.21114\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22660\t\tBest:\t0.21903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping at epoch:\t33\n"
     ]
    }
   ],
   "source": [
    "params_model = {\n",
    "    'experiment_name': 'step_1',\n",
    "    'model_name': 'deep_medic',\n",
    "    'patch_size_normal': 25,\n",
    "    'patch_size_low': 19,\n",
    "    'patch_size_out': 9,\n",
    "    'patch_low_factor': 3,\n",
    "    'run_mode': None,\n",
    "    'dataset_variant': 'npy',  # npy, nib\n",
    "    'create_numpy_dataset': False,\n",
    "    'init_timestamp': datetime.now().strftime(\"%H-%M-%S__%d-%m-%Y\")\n",
    "}\n",
    "\n",
    "params_train = {\n",
    "    'optimizer_name': 'adam',  # adam, sgd_w_momentum\n",
    "    'loss_name': 'dice',  # dice, mse, ce, dice_n_mse, dice_n_mse_n_ce\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'momentum': 0.9,\n",
    "    'use_amsgrad': True,\n",
    "    'learning_rate': 0.0002,  # 0.0002\n",
    "    'lr_scheduler_name': 'plateau',\n",
    "    'patience_lr_scheduler': 2,\n",
    "    'factor_lr_scheduler': 0.1,\n",
    "    'early_stop_condition': True,\n",
    "    'patience_early_stop': 5,\n",
    "    'early_stop_patience_counter': 0,\n",
    "    'min_epochs_to_train': 10,\n",
    "    'num_epochs': 100,\n",
    "    'save_every_epoch': True,\n",
    "\n",
    "    'save_condition': True,  # whether to save the model\n",
    "    'resume_condition': False,  # whether to resume training\n",
    "\n",
    "    'resume_dir': 'step_1__15-08-38__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100',\n",
    "    'resume_epoch': 'latest',\n",
    "\n",
    "    'batch_size': 8,  # 8\n",
    "    'batch_size_inner': 16,  # 16 (how many patches to generate per sample)\n",
    "    'train_percentage': 0.8,\n",
    "    'num_workers': 8,  # 8\n",
    "    'pin_memory': True,\n",
    "    'prefetch_factor': 2,\n",
    "    'persistent_workers': True,\n",
    "\n",
    "    'path_checkpoint': os.path.join('.', 'checkpoints'),\n",
    "    'path_checkpoint_full': '',\n",
    "    'dirname_checkpoint': '',\n",
    "    'filename_params': 'params.json',\n",
    "    'filename_logger': 'logger.txt',\n",
    "    'path_params_full': '',\n",
    "    'path_logger_full': '',\n",
    "\n",
    "    'use_elastic_deformation': False,\n",
    "    'user_affine_transformation': False,\n",
    "\n",
    "    'num_controlpoints': 20,\n",
    "    'sigma': 5,\n",
    "\n",
    "    'rotation': 10,\n",
    "    'scale': (0.90, 1.10),\n",
    "    'shear': (0.01, 0.02)\n",
    "}\n",
    "\n",
    "# instanciate model\n",
    "set_seed(1)\n",
    "params_model['experiment_name'] = 'step_1'\n",
    "model_container = ModelConainer(params_model)\n",
    "\n",
    "# train the model\n",
    "model_container.train(params_train=params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 2: Now run the same training process but now using Affine transformations and Elastic Deformations as augmentation techniques.\n",
    "Describe what the augmentation is doing, what\n",
    "parameters were used and why, and what was the outcome of the training/testing process\n",
    "when augmentation was used. Did you observe a smaller train/test performance gap? [10]\n",
    "Answer Marks:\n",
    "[ 6] 3 points for each implementation of the augmentation\n",
    "[ 2] Description of the augmentation and parameters\n",
    "[ 2] Description of the performance gains"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cd95c88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\t\tTraining starting with params:\n",
      "****************************************************************************************************\n",
      "{\n",
      "    \"params_model\": {\n",
      "        \"experiment_name\": \"step_2\",\n",
      "        \"model_name\": \"deep_medic\",\n",
      "        \"patch_size_normal\": 25,\n",
      "        \"patch_size_low\": 19,\n",
      "        \"patch_size_out\": 9,\n",
      "        \"patch_low_factor\": 3,\n",
      "        \"run_mode\": null,\n",
      "        \"dataset_variant\": \"npy\",\n",
      "        \"create_numpy_dataset\": false,\n",
      "        \"init_timestamp\": \"19-32-16__05-04-2022\",\n",
      "        \"use_elastic_deformation\": true,\n",
      "        \"num_controlpoints\": 20,\n",
      "        \"sigma\": 5,\n",
      "        \"user_affine_transformation\": true,\n",
      "        \"rotation\": 10,\n",
      "        \"scale\": [\n",
      "            0.9,\n",
      "            1.1\n",
      "        ],\n",
      "        \"shear\": [\n",
      "            0.01,\n",
      "            0.02\n",
      "        ]\n",
      "    },\n",
      "    \"params_train\": {\n",
      "        \"optimizer_name\": \"adam\",\n",
      "        \"loss_name\": \"dice\",\n",
      "        \"beta_1\": 0.9,\n",
      "        \"beta_2\": 0.999,\n",
      "        \"momentum\": 0.9,\n",
      "        \"use_amsgrad\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"lr_scheduler_name\": \"plateau\",\n",
      "        \"patience_lr_scheduler\": 2,\n",
      "        \"factor_lr_scheduler\": 0.1,\n",
      "        \"early_stop_condition\": true,\n",
      "        \"patience_early_stop\": 5,\n",
      "        \"early_stop_patience_counter\": 0,\n",
      "        \"min_epochs_to_train\": 10,\n",
      "        \"num_epochs\": 100,\n",
      "        \"save_every_epoch\": true,\n",
      "        \"save_condition\": true,\n",
      "        \"resume_condition\": false,\n",
      "        \"resume_dir\": \"14-10-08__01-04-2022__deep_medic__dice__adam__lr_0.0001__ep_50\",\n",
      "        \"resume_epoch\": \"latest\",\n",
      "        \"batch_size\": 8,\n",
      "        \"batch_size_inner\": 16,\n",
      "        \"train_percentage\": 0.8,\n",
      "        \"num_workers\": 8,\n",
      "        \"pin_memory\": true,\n",
      "        \"prefetch_factor\": 2,\n",
      "        \"persistent_workers\": true,\n",
      "        \"path_checkpoint\": \"./checkpoints\",\n",
      "        \"path_checkpoint_full\": \"./checkpoints/step_2__19-32-16__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"dirname_checkpoint\": \"step_2__19-32-16__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"filename_params\": \"params.json\",\n",
      "        \"filename_logger\": \"logger.txt\",\n",
      "        \"path_params_full\": \"./checkpoints/step_2__19-32-16__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100/params.json\",\n",
      "        \"path_logger_full\": \"./checkpoints/step_2__19-32-16__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100/logger.txt\",\n",
      "        \"use_elastic_deformation\": false,\n",
      "        \"user_affine_transformation\": false,\n",
      "        \"num_controlpoints\": 20,\n",
      "        \"sigma\": 5,\n",
      "        \"rotation\": 10,\n",
      "        \"scale\": [\n",
      "            0.9,\n",
      "            1.1\n",
      "        ],\n",
      "        \"shear\": [\n",
      "            0.01,\n",
      "            0.02\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[1 / 100]\t\tTime:\t458.88 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.54768\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.44073\t\tBest:\t0.44073\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t1\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[2 / 100]\t\tTime:\t470.35 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.44151\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.37718\t\tBest:\t0.37718\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t2\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[3 / 100]\t\tTime:\t471.91 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.40489\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.40777\t\tBest:\t0.37718\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[4 / 100]\t\tTime:\t452.36 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.40420\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.35287\t\tBest:\t0.35287\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t4\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[5 / 100]\t\tTime:\t555.81 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.35344\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.33996\t\tBest:\t0.33996\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t5\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[6 / 100]\t\tTime:\t519.59 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.38305\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.30759\t\tBest:\t0.30759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t6\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[7 / 100]\t\tTime:\t501.14 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.34097\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.35565\t\tBest:\t0.30759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[8 / 100]\t\tTime:\t496.80 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.33303\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.39398\t\tBest:\t0.30759\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[9 / 100]\t\tTime:\t593.79 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.32447\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.28083\t\tBest:\t0.28083\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t9\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[10 / 100]\t\tTime:\t601.48 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.31451\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.26961\t\tBest:\t0.26961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t10\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[11 / 100]\t\tTime:\t660.61 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.30092\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.39474\t\tBest:\t0.26961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[12 / 100]\t\tTime:\t647.37 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.31724\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.31914\t\tBest:\t0.26961\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[13 / 100]\t\tTime:\t630.68 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.30178\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25256\t\tBest:\t0.25256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t13\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[14 / 100]\t\tTime:\t518.63 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.32304\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.31157\t\tBest:\t0.25256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[15 / 100]\t\tTime:\t532.93 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28392\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.24674\t\tBest:\t0.24674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t15\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[16 / 100]\t\tTime:\t814.09 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28136\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.27104\t\tBest:\t0.24674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[17 / 100]\t\tTime:\t617.02 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28540\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.30630\t\tBest:\t0.24674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018: reducing learning rate of group 0 to 2.0000e-05.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[18 / 100]\t\tTime:\t926.29 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28499\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25281\t\tBest:\t0.24674\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[19 / 100]\t\tTime:\t951.48 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28139\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22692\t\tBest:\t0.22692\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t19\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[20 / 100]\t\tTime:\t975.75 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25543\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22926\t\tBest:\t0.22692\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[21 / 100]\t\tTime:\t790.55 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24044\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23426\t\tBest:\t0.22692\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00022: reducing learning rate of group 0 to 2.0000e-06.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[22 / 100]\t\tTime:\t890.60 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24628\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22822\t\tBest:\t0.22692\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[23 / 100]\t\tTime:\t849.21 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.23753\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.21492\t\tBest:\t0.21492\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t23\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[24 / 100]\t\tTime:\t905.11 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24468\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.21015\t\tBest:\t0.21015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t24\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[25 / 100]\t\tTime:\t656.93 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26441\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22856\t\tBest:\t0.21015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[26 / 100]\t\tTime:\t887.41 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24436\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.24515\t\tBest:\t0.21015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00027: reducing learning rate of group 0 to 2.0000e-07.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[27 / 100]\t\tTime:\t673.95 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25809\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22876\t\tBest:\t0.21015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[28 / 100]\t\tTime:\t683.02 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24454\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22038\t\tBest:\t0.21015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[29 / 100]\t\tTime:\t766.25 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26121\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22527\t\tBest:\t0.21015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping at epoch:\t29\n"
     ]
    }
   ],
   "source": [
    "params_model = {\n",
    "    'experiment_name': 'step_1',\n",
    "    'model_name': 'deep_medic',\n",
    "    'patch_size_normal': 25,\n",
    "    'patch_size_low': 19,\n",
    "    'patch_size_out': 9,\n",
    "    'patch_low_factor': 3,\n",
    "    'run_mode': None,\n",
    "    'dataset_variant': 'npy',  # npy, nib\n",
    "    'create_numpy_dataset': False,\n",
    "    'init_timestamp': datetime.now().strftime(\"%H-%M-%S__%d-%m-%Y\")\n",
    "}\n",
    "\n",
    "params_train = {\n",
    "    'optimizer_name': 'adam',  # adam, sgd_w_momentum\n",
    "    'loss_name': 'dice',  # dice, mse, ce, dice_n_mse, dice_n_mse_n_ce\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'momentum': 0.9,\n",
    "    'use_amsgrad': True,\n",
    "    'learning_rate': 0.0002,  # 0.0002\n",
    "    'lr_scheduler_name': 'plateau',\n",
    "    'patience_lr_scheduler': 2,\n",
    "    'factor_lr_scheduler': 0.1,\n",
    "    'early_stop_condition': True,\n",
    "    'patience_early_stop': 5,\n",
    "    'early_stop_patience_counter': 0,\n",
    "    'min_epochs_to_train': 10,\n",
    "    'num_epochs': 100,\n",
    "    'save_every_epoch': True,\n",
    "\n",
    "    'save_condition': True,  # whether to save the model\n",
    "    'resume_condition': False,  # whether to resume training\n",
    "\n",
    "    'resume_dir': 'step_2__19-32-16__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100',\n",
    "    'resume_epoch': 'latest',\n",
    "\n",
    "    'batch_size': 8,  # 8\n",
    "    'batch_size_inner': 16,  # 16 (how many patches to generate per sample)\n",
    "    'train_percentage': 0.8,\n",
    "    'num_workers': 8,  # 8\n",
    "    'pin_memory': True,\n",
    "    'prefetch_factor': 2,\n",
    "    'persistent_workers': True,\n",
    "\n",
    "    'path_checkpoint': os.path.join('.', 'checkpoints'),\n",
    "    'path_checkpoint_full': '',\n",
    "    'dirname_checkpoint': '',\n",
    "    'filename_params': 'params.json',\n",
    "    'filename_logger': 'logger.txt',\n",
    "    'path_params_full': '',\n",
    "    'path_logger_full': '',\n",
    "\n",
    "    'use_elastic_deformation': False,\n",
    "    'user_affine_transformation': False,\n",
    "\n",
    "    'num_controlpoints': 20,\n",
    "    'sigma': 5,\n",
    "\n",
    "    'rotation': 10,\n",
    "    'scale': (0.90, 1.10),\n",
    "    'shear': (0.01, 0.02)\n",
    "}\n",
    "\n",
    "# instanciate model\n",
    "set_seed(1)\n",
    "params_model['experiment_name'] = 'step_2'\n",
    "\n",
    "params_model['use_elastic_deformation'] = True\n",
    "params_model['num_controlpoints'] = 20\n",
    "params_model['sigma'] = 5\n",
    "\n",
    "params_model['user_affine_transformation'] = True\n",
    "params_model['rotation'] = 10\n",
    "params_model['scale'] = (0.90, 1.10)\n",
    "params_model['shear'] = (0.01, 0.02)\n",
    "\n",
    "model_container = ModelConainer(params_model)\n",
    "\n",
    "# train the model\n",
    "model_container.train(params_train=params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 4: Vessels are small and thin. This commonly results in a disconnected vessel three. Predicting distance maps (e.g. https://arxiv.org/pdf/1908.05099.pdf) is a good auxiliary\n",
    "task to force a network to understand vessel geometry. Implement this auxiliary task and\n",
    "assess the performance with and without the auxiliary task. [20]\n",
    "Answer Marks:\n",
    "[13] Correct implementation of the task\n",
    "[ 2] Code comments\n",
    "[ 5] Description of the performance gains"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DICE + MSE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "\t\tTraining starting with params:\n",
      "****************************************************************************************************\n",
      "{\n",
      "    \"params_model\": {\n",
      "        \"experiment_name\": \"step_4\",\n",
      "        \"model_name\": \"deep_medic\",\n",
      "        \"patch_size_normal\": 25,\n",
      "        \"patch_size_low\": 19,\n",
      "        \"patch_size_out\": 9,\n",
      "        \"patch_low_factor\": 3,\n",
      "        \"run_mode\": null,\n",
      "        \"dataset_variant\": \"npy\",\n",
      "        \"create_numpy_dataset\": false,\n",
      "        \"init_timestamp\": \"09-03-12__06-04-2022\",\n",
      "        \"loss_name\": \"dice_n_mse\",\n",
      "        \"use_elastic_deformation\": true,\n",
      "        \"num_controlpoints\": 20,\n",
      "        \"sigma\": 5,\n",
      "        \"user_affine_transformation\": true,\n",
      "        \"rotation\": 10,\n",
      "        \"scale\": [\n",
      "            0.9,\n",
      "            1.1\n",
      "        ],\n",
      "        \"shear\": [\n",
      "            0.01,\n",
      "            0.02\n",
      "        ]\n",
      "    },\n",
      "    \"params_train\": {\n",
      "        \"optimizer_name\": \"adam\",\n",
      "        \"loss_name\": \"dice\",\n",
      "        \"beta_1\": 0.9,\n",
      "        \"beta_2\": 0.999,\n",
      "        \"momentum\": 0.9,\n",
      "        \"use_amsgrad\": true,\n",
      "        \"learning_rate\": 0.0002,\n",
      "        \"lr_scheduler_name\": \"plateau\",\n",
      "        \"patience_lr_scheduler\": 2,\n",
      "        \"factor_lr_scheduler\": 0.1,\n",
      "        \"early_stop_condition\": true,\n",
      "        \"patience_early_stop\": 5,\n",
      "        \"early_stop_patience_counter\": 0,\n",
      "        \"min_epochs_to_train\": 10,\n",
      "        \"num_epochs\": 100,\n",
      "        \"save_every_epoch\": true,\n",
      "        \"save_condition\": true,\n",
      "        \"resume_condition\": false,\n",
      "        \"resume_dir\": \"step_2__19-32-16__05-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"resume_epoch\": \"latest\",\n",
      "        \"batch_size\": 8,\n",
      "        \"batch_size_inner\": 16,\n",
      "        \"train_percentage\": 0.8,\n",
      "        \"num_workers\": 8,\n",
      "        \"pin_memory\": true,\n",
      "        \"prefetch_factor\": 2,\n",
      "        \"persistent_workers\": true,\n",
      "        \"path_checkpoint\": \"./checkpoints\",\n",
      "        \"path_checkpoint_full\": \"./checkpoints/step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"dirname_checkpoint\": \"step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"filename_params\": \"params.json\",\n",
      "        \"filename_logger\": \"logger.txt\",\n",
      "        \"path_params_full\": \"./checkpoints/step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100/params.json\",\n",
      "        \"path_logger_full\": \"./checkpoints/step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100/logger.txt\",\n",
      "        \"use_elastic_deformation\": false,\n",
      "        \"user_affine_transformation\": false,\n",
      "        \"num_controlpoints\": 20,\n",
      "        \"sigma\": 5,\n",
      "        \"rotation\": 10,\n",
      "        \"scale\": [\n",
      "            0.9,\n",
      "            1.1\n",
      "        ],\n",
      "        \"shear\": [\n",
      "            0.01,\n",
      "            0.02\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[1 / 100]\t\tTime:\t456.72 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.54791\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.43348\t\tBest:\t0.43348\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t1\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[2 / 100]\t\tTime:\t495.79 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.44145\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.37291\t\tBest:\t0.37291\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t2\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[3 / 100]\t\tTime:\t512.69 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.40581\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.40209\t\tBest:\t0.37291\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[4 / 100]\t\tTime:\t562.41 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.39930\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.34903\t\tBest:\t0.34903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t4\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[5 / 100]\t\tTime:\t626.50 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.34937\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.30827\t\tBest:\t0.30827\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t5\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[6 / 100]\t\tTime:\t753.72 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.37694\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.29229\t\tBest:\t0.29229\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t6\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[7 / 100]\t\tTime:\t705.96 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.34502\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.38347\t\tBest:\t0.29229\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[8 / 100]\t\tTime:\t736.27 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.34238\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.33099\t\tBest:\t0.29229\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[9 / 100]\t\tTime:\t758.10 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.31946\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.27836\t\tBest:\t0.27836\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t9\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[10 / 100]\t\tTime:\t948.15 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.31934\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.28000\t\tBest:\t0.27836\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[11 / 100]\t\tTime:\t908.63 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.30784\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.29417\t\tBest:\t0.27836\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[12 / 100]\t\tTime:\t1211.46 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.30960\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25787\t\tBest:\t0.25787\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t12\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[13 / 100]\t\tTime:\t1028.96 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.29819\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.30882\t\tBest:\t0.25787\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[14 / 100]\t\tTime:\t1026.18 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.33662\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.32230\t\tBest:\t0.25787\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[15 / 100]\t\tTime:\t949.85 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28269\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.24181\t\tBest:\t0.24181\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t15\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[16 / 100]\t\tTime:\t934.00 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.28256\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.34878\t\tBest:\t0.24181\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[17 / 100]\t\tTime:\t943.54 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.29130\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23611\t\tBest:\t0.23611\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t17\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[18 / 100]\t\tTime:\t983.46 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.27954\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25617\t\tBest:\t0.23611\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[19 / 100]\t\tTime:\t934.92 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.29714\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25195\t\tBest:\t0.23611\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[20 / 100]\t\tTime:\t1177.66 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.26778\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23050\t\tBest:\t0.23050\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t20\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[21 / 100]\t\tTime:\t920.18 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25666\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.25889\t\tBest:\t0.23050\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[22 / 100]\t\tTime:\t952.53 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25525\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.28110\t\tBest:\t0.23050\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[23 / 100]\t\tTime:\t871.72 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25715\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22780\t\tBest:\t0.22780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t23\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[24 / 100]\t\tTime:\t837.13 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25507\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23877\t\tBest:\t0.22780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[25 / 100]\t\tTime:\t928.81 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.27627\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.26304\t\tBest:\t0.22780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00026: reducing learning rate of group 0 to 2.0000e-05.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[26 / 100]\t\tTime:\t885.86 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25396\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23466\t\tBest:\t0.22780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[27 / 100]\t\tTime:\t898.64 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.25077\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.23698\t\tBest:\t0.22780\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[28 / 100]\t\tTime:\t806.23 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.22900\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.21811\t\tBest:\t0.21811\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t28\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[29 / 100]\t\tTime:\t847.17 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24899\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.21365\t\tBest:\t0.21365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "**********\tNew best model saved at:\t29\t**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[30 / 100]\t\tTime:\t872.10 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.24870\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22354\t\tBest:\t0.21365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[31 / 100]\t\tTime:\t819.09 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.22305\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22730\t\tBest:\t0.21365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00032: reducing learning rate of group 0 to 2.0000e-06.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[32 / 100]\t\tTime:\t792.36 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.23197\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22747\t\tBest:\t0.21365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[33 / 100]\t\tTime:\t755.72 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.20683\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.22076\t\tBest:\t0.21365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t[34 / 100]\t\tTime:\t766.89 s\n",
      "\tTRAIN\t\t-->\t\tLoss Total:\t\t0.22011\n",
      "\tVAL\t\t\t-->\t\tLoss Total:\t\t0.21798\t\tBest:\t0.21365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Early stopping at epoch:\t34\n"
     ]
    }
   ],
   "source": [
    "params_model = {\n",
    "    'experiment_name': 'step_1',\n",
    "    'model_name': 'deep_medic',\n",
    "    'patch_size_normal': 25,\n",
    "    'patch_size_low': 19,\n",
    "    'patch_size_out': 9,\n",
    "    'patch_low_factor': 3,\n",
    "    'run_mode': None,\n",
    "    'dataset_variant': 'npy',  # npy, nib\n",
    "    'create_numpy_dataset': False,\n",
    "    'init_timestamp': datetime.now().strftime(\"%H-%M-%S__%d-%m-%Y\")\n",
    "}\n",
    "\n",
    "params_train = {\n",
    "    'optimizer_name': 'adam',  # adam, sgd_w_momentum\n",
    "    'loss_name': 'dice',  # dice, mse, ce, dice_n_mse, dice_n_mse_n_ce\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'momentum': 0.9,\n",
    "    'use_amsgrad': True,\n",
    "    'learning_rate': 0.0002,  # 0.0002\n",
    "    'lr_scheduler_name': 'plateau',\n",
    "    'patience_lr_scheduler': 2,\n",
    "    'factor_lr_scheduler': 0.1,\n",
    "    'early_stop_condition': True,\n",
    "    'patience_early_stop': 5,\n",
    "    'early_stop_patience_counter': 0,\n",
    "    'min_epochs_to_train': 10,\n",
    "    'num_epochs': 100,\n",
    "    'save_every_epoch': True,\n",
    "\n",
    "    'save_condition': True,  # whether to save the model\n",
    "    'resume_condition': False,  # whether to resume training\n",
    "\n",
    "    'resume_dir': 'step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100',\n",
    "    'resume_epoch': 'latest',\n",
    "\n",
    "    'batch_size': 8,  # 8\n",
    "    'batch_size_inner': 16,  # 16 (how many patches to generate per sample)\n",
    "    'train_percentage': 0.8,\n",
    "    'num_workers': 8,  # 8\n",
    "    'pin_memory': True,\n",
    "    'prefetch_factor': 2,\n",
    "    'persistent_workers': True,\n",
    "\n",
    "    'path_checkpoint': os.path.join('.', 'checkpoints'),\n",
    "    'path_checkpoint_full': '',\n",
    "    'dirname_checkpoint': '',\n",
    "    'filename_params': 'params.json',\n",
    "    'filename_logger': 'logger.txt',\n",
    "    'path_params_full': '',\n",
    "    'path_logger_full': '',\n",
    "\n",
    "    'use_elastic_deformation': False,\n",
    "    'user_affine_transformation': False,\n",
    "\n",
    "    'num_controlpoints': 20,\n",
    "    'sigma': 5,\n",
    "\n",
    "    'rotation': 10,\n",
    "    'scale': (0.90, 1.10),\n",
    "    'shear': (0.01, 0.02)\n",
    "}\n",
    "\n",
    "# instanciate model\n",
    "set_seed(1)\n",
    "params_model['experiment_name'] = 'step_4'\n",
    "params_model['loss_name'] = 'dice_n_mse'\n",
    "\n",
    "params_model['use_elastic_deformation'] = True\n",
    "params_model['num_controlpoints'] = 20\n",
    "params_model['sigma'] = 5\n",
    "\n",
    "params_model['user_affine_transformation'] = True\n",
    "params_model['rotation'] = 10\n",
    "params_model['scale'] = (0.90, 1.10)\n",
    "params_model['shear'] = (0.01, 0.02)\n",
    "\n",
    "model_container = ModelConainer(params_model)\n",
    "\n",
    "# train the model\n",
    "model_container.train(params_train=params_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### DCIE + MSE + CE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_model = {\n",
    "    'experiment_name': 'step_1',\n",
    "    'model_name': 'deep_medic',\n",
    "    'patch_size_normal': 25,\n",
    "    'patch_size_low': 19,\n",
    "    'patch_size_out': 9,\n",
    "    'patch_low_factor': 3,\n",
    "    'run_mode': None,\n",
    "    'dataset_variant': 'npy',  # npy, nib\n",
    "    'create_numpy_dataset': False,\n",
    "    'init_timestamp': datetime.now().strftime(\"%H-%M-%S__%d-%m-%Y\")\n",
    "}\n",
    "\n",
    "params_train = {\n",
    "    'optimizer_name': 'adam',  # adam, sgd_w_momentum\n",
    "    'loss_name': 'dice',  # dice, mse, ce, dice_n_mse, dice_n_mse_n_ce\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'momentum': 0.9,\n",
    "    'use_amsgrad': True,\n",
    "    'learning_rate': 0.0002,  # 0.0002\n",
    "    'lr_scheduler_name': 'plateau',\n",
    "    'patience_lr_scheduler': 2,\n",
    "    'factor_lr_scheduler': 0.1,\n",
    "    'early_stop_condition': True,\n",
    "    'patience_early_stop': 5,\n",
    "    'early_stop_patience_counter': 0,\n",
    "    'min_epochs_to_train': 10,\n",
    "    'num_epochs': 100,\n",
    "    'save_every_epoch': True,\n",
    "\n",
    "    'save_condition': True,  # whether to save the model\n",
    "    'resume_condition': False,  # whether to resume training\n",
    "\n",
    "    'resume_dir': 'step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100',\n",
    "    'resume_epoch': 'latest',\n",
    "\n",
    "    'batch_size': 8,  # 8\n",
    "    'batch_size_inner': 16,  # 16 (how many patches to generate per sample)\n",
    "    'train_percentage': 0.8,\n",
    "    'num_workers': 8,  # 8\n",
    "    'pin_memory': True,\n",
    "    'prefetch_factor': 2,\n",
    "    'persistent_workers': True,\n",
    "\n",
    "    'path_checkpoint': os.path.join('.', 'checkpoints'),\n",
    "    'path_checkpoint_full': '',\n",
    "    'dirname_checkpoint': '',\n",
    "    'filename_params': 'params.json',\n",
    "    'filename_logger': 'logger.txt',\n",
    "    'path_params_full': '',\n",
    "    'path_logger_full': '',\n",
    "\n",
    "    'use_elastic_deformation': False,\n",
    "    'user_affine_transformation': False,\n",
    "\n",
    "    'num_controlpoints': 20,\n",
    "    'sigma': 5,\n",
    "\n",
    "    'rotation': 10,\n",
    "    'scale': (0.90, 1.10),\n",
    "    'shear': (0.01, 0.02)\n",
    "}\n",
    "\n",
    "# instanciate model\n",
    "set_seed(1)\n",
    "params_model['experiment_name'] = 'step_4'\n",
    "params_model['loss_name'] = 'dice_n_mse_n_ce'\n",
    "\n",
    "model_container = ModelConainer(params_model)\n",
    "\n",
    "# train the model\n",
    "model_container.train(params_train=params_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# STEP 6: Ensemble all the models by averaging their probalility.\n",
    "You can achieve this by either\n",
    "sharing the models themselves among the team, or by sharing the probabilistic outputs of\n",
    "the models. Comment on the algorithmic performance of the ensemble compared to your\n",
    "own method. [10]\n",
    "Answer Marks:\n",
    "[ 5] Implementation of the average ensemble\n",
    "[ 5] Describe the differences in performance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the outputs from the inference: I am using the model from the trained mdoel from step 4, which uses DICE loss as well as the distance maps"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from epoch:\t30\n",
      "****************************************************************************************************\n",
      "\t\tInference starting with params:\n",
      "****************************************************************************************************\n",
      "{\n",
      "    \"params_model\": {\n",
      "        \"experiment_name\": \"step_4\",\n",
      "        \"model_name\": \"deep_medic\",\n",
      "        \"patch_size_normal\": 25,\n",
      "        \"patch_size_low\": 19,\n",
      "        \"patch_size_out\": 9,\n",
      "        \"patch_low_factor\": 3,\n",
      "        \"run_mode\": null,\n",
      "        \"dataset_variant\": \"npy\",\n",
      "        \"create_numpy_dataset\": false,\n",
      "        \"init_timestamp\": \"09-03-12__06-04-2022\",\n",
      "        \"loss_name\": \"dice_n_mse\",\n",
      "        \"use_elastic_deformation\": true,\n",
      "        \"num_controlpoints\": 20,\n",
      "        \"sigma\": 5,\n",
      "        \"user_affine_transformation\": true,\n",
      "        \"rotation\": 10,\n",
      "        \"scale\": [\n",
      "            0.9,\n",
      "            1.1\n",
      "        ],\n",
      "        \"shear\": [\n",
      "            0.01,\n",
      "            0.02\n",
      "        ]\n",
      "    },\n",
      "    \"params_inference\": {\n",
      "        \"loss_name\": \"dice\",\n",
      "        \"batch_size\": 1,\n",
      "        \"train_percentage\": 0.8,\n",
      "        \"num_workers\": 0,\n",
      "        \"pin_memory\": false,\n",
      "        \"prefetch_factor\": 2,\n",
      "        \"persistent_workers\": false,\n",
      "        \"resume_dir\": \"step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100\",\n",
      "        \"resume_epoch\": \"best\",\n",
      "        \"path_checkpoint\": \".\\\\checkpoints\",\n",
      "        \"path_checkpoint_full\": \"\",\n",
      "        \"dirname_checkpoint\": \"\"\n",
      "    }\n",
      "}\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31: \t30.npy\tLoss DICE:\t0.39051\tLoss MSE:\t0.03766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32: \t31.npy\tLoss DICE:\t0.38103\tLoss MSE:\t0.03800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33: \t32.npy\tLoss DICE:\t0.57124\tLoss MSE:\t0.03101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34: \t33.npy\tLoss DICE:\t0.35475\tLoss MSE:\t0.03918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35: \t34.npy\tLoss DICE:\t0.50300\tLoss MSE:\t0.03211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36: \t35.npy\tLoss DICE:\t0.40539\tLoss MSE:\t0.04297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37: \t36.npy\tLoss DICE:\t0.52183\tLoss MSE:\t0.03593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38: \t37.npy\tLoss DICE:\t0.51899\tLoss MSE:\t0.03090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39: \t38.npy\tLoss DICE:\t0.38306\tLoss MSE:\t0.03462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40: \t39.npy\tLoss DICE:\t0.58272\tLoss MSE:\t0.04245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41: \t40.npy\tLoss DICE:\t0.59060\tLoss MSE:\t0.04261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42: \t41.npy\tLoss DICE:\t0.45669\tLoss MSE:\t0.03946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43: \t42.npy\tLoss DICE:\t0.44385\tLoss MSE:\t0.02936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44: \t43.npy\tLoss DICE:\t0.21941\tLoss MSE:\t0.02203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45: \t44.npy\tLoss DICE:\t0.55297\tLoss MSE:\t0.03464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46: \t45.npy\tLoss DICE:\t0.50854\tLoss MSE:\t0.02983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47: \t46.npy\tLoss DICE:\t0.31312\tLoss MSE:\t0.02055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48: \t47.npy\tLoss DICE:\t0.53860\tLoss MSE:\t0.04269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49: \t48.npy\tLoss DICE:\t0.42796\tLoss MSE:\t0.02662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50: \t49.npy\tLoss DICE:\t0.51611\tLoss MSE:\t0.03639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51: \t50.npy\tLoss DICE:\t0.40107\tLoss MSE:\t0.02452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52: \t51.npy\tLoss DICE:\t0.37862\tLoss MSE:\t0.03711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53: \t52.npy\tLoss DICE:\t0.49972\tLoss MSE:\t0.04058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54: \t53.npy\tLoss DICE:\t0.44210\tLoss MSE:\t0.03262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55: \t54.npy\tLoss DICE:\t0.50054\tLoss MSE:\t0.03323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56: \t55.npy\tLoss DICE:\t0.43322\tLoss MSE:\t0.03024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57: \t56.npy\tLoss DICE:\t0.38116\tLoss MSE:\t0.02084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58: \t57.npy\tLoss DICE:\t0.54903\tLoss MSE:\t0.03079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59: \t58.npy\tLoss DICE:\t0.52512\tLoss MSE:\t0.03185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60: \t59.npy\tLoss DICE:\t0.34767\tLoss MSE:\t0.01777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61: \t60.npy\tLoss DICE:\t0.39607\tLoss MSE:\t0.03667\n"
     ]
    }
   ],
   "source": [
    "params_model = {\n",
    "    'experiment_name': 'step_1',\n",
    "    'model_name': 'deep_medic',\n",
    "    'patch_size_normal': 25,\n",
    "    'patch_size_low': 19,\n",
    "    'patch_size_out': 9,\n",
    "    'patch_low_factor': 3,\n",
    "    'run_mode': None,\n",
    "    'dataset_variant': 'npy',  # npy, nib\n",
    "    'create_numpy_dataset': False,\n",
    "    'init_timestamp': datetime.now().strftime(\"%H-%M-%S__%d-%m-%Y\")\n",
    "}\n",
    "\n",
    "params_inference = {\n",
    "    'loss_name': 'dice',\n",
    "    'batch_size': 1,\n",
    "    'train_percentage': 0.8,\n",
    "    'num_workers': 0,  # 8\n",
    "    'pin_memory': False,\n",
    "    'prefetch_factor': 2,\n",
    "    'persistent_workers': False,\n",
    "\n",
    "    'resume_dir': 'step_4__09-03-12__06-04-2022__deep_medic__dice__adam__lr_0.0002__ep_100',\n",
    "    'resume_epoch': 'best',\n",
    "    'path_checkpoint': os.path.join('.', 'checkpoints'),\n",
    "    'path_checkpoint_full': '',\n",
    "    'dirname_checkpoint': '',\n",
    "}\n",
    "\n",
    "params_model['loss_name'] = 'dice_n_mse'\n",
    "\n",
    "# instanciate model\n",
    "set_seed(1)\n",
    "model_container = ModelConainer(params_model)\n",
    "\n",
    "# inference\n",
    "model_container.inference(params_inference=params_inference)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}